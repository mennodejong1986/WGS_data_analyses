#!/bin/bash
# This script is for rapid genotype calling/filtering using bcftools mpileup, call, norm, filter and view.
# Input: bam files (indexed with samtools), one bam file per individual. 
# Output: vcf file, containing genotype information for all individuals combined.

# The mpileup command is by default run with the flags --min-MQ 20 and -min-BQ 13.
# Bam-files should contain readgroups (e.g. using PICARD addorreplace readgroups) and should have been indexed (using samtools index).
# It is also recommended that duplicate reads have been removed (e.g. using PICARD markduplicates).
# For more info, see script: FASTQ2BAM_bwaloop.sh.  

# In principle you could invoke all steps in one go by setting all flags in the user-defined section to TRUE.
# However, do not do so (!!). 
# Instead, I recommend to proceed stepwise, and to only move on to the next step only after checking if all intermediate output files look okay and no errors have occurred.
# Also, based on depth and heterozygosity statistics generated by this script, you might want to customize filter settings (as explained below). 

# Also, make sure you have enough storage space.
# As an indication: 120 mammalian samples (say: 2.5 Gb genome size) typically result in a vcf-file with a total of approximately 300 Gb. 
# Because the analyses are run on subsets, and because eventually you may have to combine the subsets into one single vcf file, you would need at least 600 Gb free space.   

# Four main output files:
# - PREFIX.globalfilter.thin0.vcf.gz			# gVCF file, which contains all sites (monomorphic and polymorphic) retained after global filtering. To be used as input for Darwindow, and for haploblock analyses (?).
# - PREFIX.globalfilter.thinX.vcf.gz			# randomly thinned gvcf-file. To be used as input for VCF_calcdist.sh script.
# - PREFIX.mysnps.thinX.vcf.gz					# contains all variant sites (SNP data) present in the input files. Input can be either thinned or nonthinned (i.e., thinning factor is 0). Needed for haploblock analyses or as input for SambaR.
# - PREFIX.mysnps.thinX_X.missfilter.vcf.gz		# contains subset of variant sites (after additional thinning and/or filtering on missing data).

# Note: this script generates a vcf-file containing information for all samples combined.
# In other words: it does not generate a vcf-file for each single individual.
# Therefore, if indeed running the analysis for multiple samples (i.e. multiple bam files), it is crucial that each bam file contains a unique SM tag in the header of the bam file.
# If you do not have unique SM tags, you will end up with a vcf file containing information for a single individual only.
# Normally, unique SM tags are added with picard AddOrReplaceReadGroups.
# An alternative easy and fast way to edit the SM tags is samtools reheader. Say you want to change the SM tag from 'sample' to 'human_1', the command would be:
# samtools view -H file.bam > mybamheader.txt
# sed "s/SM:sample/SM:human_1/g" mybamheader.txt
# samtools reheader mybamheader.txt file.bam > file.newheader.bam

# Note: although of course you are free to give it a try, is might not be advisable to run this script in a conda environment.
# In a conda environment, I typically run into the error: File size limit exceeded (core dumped).

# In case you are interested (but purely optional), scroll down to bottom of script for more information on:
# - method used by this script to divide the genome in sections, in order to run bcftools mpileup parallel, and thus speed up calculations 
# - bcftools settings used by this script
# - general information on the different types of quality scores 
# - general information on the calculations underlying genotype calling 

# QUICK START GUIDE
# To create the PREFIX.globalfilter.vcf.gz file (needed as input for Darwindow), go through the following steps:
# 0. make the script an executable (chmod +x BAM2VCF_run_mpileup_parallel_HIGHWAY.sh) and remove potential unwanted hidden characters using dos2unix command (dos2unix BAM2VCF_run_mpileup_parallel_HIGHWAY.sh)
# 1. In the user-defined section, define paths to software, reference genome and a txt file which lists the input bam files. 
# 2. In the user-defined section, define the desired number of sets (i.e, number of parallel runs) and the prefix of the output bcf file.
# 3. Set the flag 'createbed' to TRUE (leave all other FALSE) and execute the script (i.e., type on the command line: ./BAM2VCF_run_mpileup_parallel_HIGHWAY.)
# 4. Observe the file 'bedfiles.nrsites.txt' to see if sites are relatively equally distributed, and if there are no empty bed-files (if there are empty bed files, remove them.) Optionally, split big files in a and b (e.g. mybed9a.txt and mybed9b.txt).
# 5. If everything looks okay, set the flag 'createset' to FALSE, set the flag 'runpipeline' to TRUE, and then execute the script. 
#	 Note, this will run bcftools mpileup, norm, call and filter (mask) all at once. This can take hours to days, depending on data size, and the number of subsets.	
# 6. Once finished, set the flag 'runpipeline' to FALSE, set the flag 'calcdepth' to TRUE, and execute the script. 
# 7. Afterwards, use the R script 'BAM2VCF_plotdepth.inR.txt' in R to create plots depicting the depth distribution. 
# 8. Based on information acquired at step 6 and 7, set appropriate values for 'mindepth' and 'maxdepth', and then run the 'globalfilter' step.
# 9. The output will be a file called 'PREFIX.globalfilter.vcf.gz'. This is a gVCF-file, which contains information for polymorphic AND monomorphic sites. This can be used as input to the scripts VCF_darwindow.sh and VCF_calcdist.sh. 

# The create the PREFIX.mysnps.thinned.vcf.gz file (needed as input for SambaR), continue with the following steps: 
# 10. If you want to generate SNP data set, continue with the steps selectbiallelic and missfilter (edit minalleles flag) to generate a vcf-file with polymorphic sites (i.e., biallelic SNPs) only. 
# 11. Continue with the steps thinvcf (edit thinbp flag) and convert2ped to create a thinned dataset stored in PED/MAP and RAW/BIM format, which can be used as input for SambaR.   

# IF NEEDED, INSTALL LATEST VERSION OF BCFTOOLS (or SAMTOOLS):
# download from htslib.org website
# tar -xvf bcftools-1.19.tar.bz2
# cd bcftools-1.19
# ./configure --prefix=/home/mdejong/software/bcftools/bcftools-1.19
# make
# make install



########################  USER-DEFINED SECTION ##################################

# SOFTWARE
SAMTOOLS=/opt/software/samtools-1.20/bin/samtools							# Path to samtools executable
BCFTOOLS=/opt/software/bcftools-1.20/bin/bcftools                          	# Path to bcftools executable, should be a fairly recent version of bcftools (1.12 or newer), because older versions do not contain bcftools call --group-samples option
VCFTOOLS=/opt/software/vcftools/vcftools_0.1.17/bin/vcftools                # Path to vcftools executable (needed for depth calculations and for thinning)
PLINK=/home/mdejong/software/plinkv2020-09-21/plink                         # Path to plink executable (optional, only needed for convert2ped step)

# INPUT FILES
#REFERENCE=/path/to/genome/referencegenome.fa		# Path to unzipped reference genome. Genome should be indexed with samtools faidx command (which is automatically done by 'createbed' step.
REFERENCE=/home/mdejong/bearproject/refgenome/brownbear_chrom/ASM358476v1_HiC.fasta # CUSTOMISE #
BAMFILES=mybamfiles.txt								# CUSTOMISE # A txt file with (full path and) names of input bam files, one per line. Bam-files should already have been indexed with samtools index command.
POPFILE=mygroupfile.txt								# CUSTOMISE # Required for callgenotypes step; tab separated txt-file with two columns (no header): sample name and population name. 
													# Note: if you run the add read group command, this should not correspond to bam-file name, but to SM-tag in bam-file (specified with addreadgroup-command). 
													# For example: 'sample1.sorted.bam' could become simply 'sample1' (check the SM tag).
													# If the names do not correspond, you run into the ERROR: 'Could not parse the file, no matching samples found:'
													# Two possible strategies:
													# 1. assign each individual to its own group (e.g. sample1 to sample1, sample2 to sample2, etc).Â 
													# 2. assign each individual to the population it belongs to (e.g. sample1 and sample2 to pop1, and sample3 and sample4 to pop2, etc.)
													# These two strategies both have their pros and cons. 
													# You may only use the second option if you are absolutely sure about the population assigment, and if the populations are panmictic. If not, you will bias the outcome ('output being input').
													# To avoid 'output=uninput' (i.e., steering the output in a certain direction), and to make the outcome independent from your sample collection (e.g., adding more samples won't affect the genotype calls of individual A), I recommend option 1. 
													# In other words, group all samples apart, meaning: column 2 identical to column 1.
													# However, for completeness I should add that simulation studies indicate that if option 2 is applied correctly, you will have better sensitivity and specificity (i.e., less genotype call errors). See:
													# Liu et al. 2022, Comparison of seven SNP calling pipelines for the next-generation sequencing data of chickens
													# Still, I always use option 1, and while this might result in a higher overall genotyping error rate, at least I need experience any bias: the results always make sense.			
# GENERAL SETTINGS
PREFIX=Camelids					# Desired prefix of output files. Should NOT contain a period (.), otherwise the sorting of the subset files will be corrupted. 
PLOIDY=2                        # Only used if setploidy is set to FALSE (default). Set to 1 for mt-DNA data and Y-chromosomal data, or leave at 2 (default) for autosomal data (assuming diploid organisms).
setploidy=FALSE                 # Only set to true if you want to define ploidy level dependent on gender of sample (e.g. for Y-chrom or X-chrom). If true, define ploidyfile and samplefile:
PLOIDYFILE=myploidyfile.txt		# Needed if setploidy=TRUE for callgenotypes step; tab-separated txt-file with 5 columns: chrom, start, end, gender, ploidy
SEXFILE=allsexinfo.txt   		# Needed if setploidy=TRUE for callgenotypes step; tab-separated txt-file with 2 columns (no header): sample name and gender (M or F)

# CONTROL PANEL
# from here onwards, traffic light control panel to run analyses step by step
## double hashtag indicates an analysis step
# single hashtag indicates a setting for the above analysis step

# CREATE BED-FILES:
createbed=FALSE			## Divide genome in subsets
NRSETS=30				# Number of subdivisions (this is basically the number of commands you want to run parallel). 
						# Note: sometimes when running 'createbed' you might get the error: Error: mybed.txt: No such file or directory. In that case choose another (lower?) value for NRSETS and try again.
						# If possible (meaning: in case you have identified them), remove the scaffolds of the X-chrom and Y-chrom and run the analyses on these scaffolds separately, with different ploidy settings.
myregion=mybed2.txt		# CUSTOMISE # After createbed step, specify here the name of a non-empty bed-file. This will be used in subsequent steps to check whether all expected files are present.

# GENOTYPE CALLING:
runpipeline=FALSE		## Main step, runs all at once: bcftools mpileup, norm , call and filter (mask). Use 'maskdepth' and 'maxsampledepth' to specify masking threshold and maximum number of reads to be considered per sample.  
maskdepth=5				# By default, genotypes with read depth below 5 are masked.
maxsampledepth=250		# For mtDNA, set to 100000.	
combinevcf=FALSE		## Optional, and never needed, as globalfilter-step can also run on vcf file subsets (which is much faster).	

# DEPTH CALCULATIONS:
calcdepth=FALSE			## Calculate distribution of depths across sites for a subset of genome (use 'bedfile' and 'thinbp2' to specify ). This depth information is needed to set the correct mindepth and maxdepth threshold when running the next step (globalfilter).
depththinbp=10000		# Specify here the thinning factor.     

# SITE FILTERING:
globalfilter=FALSE		## Runs either on vcf file subsets or combined vcf file. Use the flags 'removeindels', 'min_alleles', 'mindepth' and 'maxdepth' to customize filter, in case of the latter two using information obtained from the calcdepth step (which can be visualised using BAM2VCF_plotdepth.R script.			
mindepth=80				# CUSTOMISE. Use BAM2VCF_calcdepth output and BAM2VCF_plotdepth.R scripts to determine the value. Expected value is around 0.6x overall meandepth (e.g: if mean depth per sample is 10, and you have 100 samples, then: 10*100*0.6 = 600). 
maxdepth=250			# CUSTOMISE. Use BAM2VCF_calcdepth output and BAM2VCF_plotdepth.R scripts to determine the value. Expected value is around 1.5x overall meandepth (e.g: if mean depth per sample is 10, and you have 100 samples, then: 10*100*1.5 = 1500). 
min_alleles=0			# recommendation: if possible keep to zero. Only a min_alleles threshold, if really needed. That is, if certain samples do not behave (deviate from ultrametricity), then set to approximately 1x nrsamples in the case of diploid data. 
site_quality=0			# recommendation: if possible keep to zero. Only use a site quality filter, if really needed. If certain samples do not behave (deviate from ultrametricity, set to 15, or max 20. Do NOT set higher than 20. This causes a disproportionate loss of monomorphic sites relative to polymorphic sites, and hence biases He, pi and Dxy-estimates. 
removeindels=TRUE		# this is an option of the globalfilter step. By default set to TRUE. All downstream analyses which I run, require that indels have been removed.    
combinevcf2=FALSE		## Optional, as next step can also run on vcf file subsets. Needed though as input for Darwindow ('PREFIX.globalfilter.vcf.gz').

# THIN DATA (OPTIONAL):
thinallsites=FALSE		## Runs either on vcf file subsets or combined vcf file.
thinallbp=100			# Select one site every n basepair (either monomorphic or polymorphic)
combinethin=FALSE		## Combine thinned file. Not really needed for anything really.
countsites=FALSE		## Count number of sites prior to variant selection

# SNP SELECTION:
selectvariants=FALSE	## Runs either on vcf file subsets or combined vcf file. Use the flags 'thinfactor' and 'biallelic' to specify input and output.
thinfactor=0			# Which thinning factor was used to create the input files? If wanting to extract SNPs from unthinned data, then leave to 0. If from the thinned dataset, define same value as for thinallbp.
biallelic=FALSE			# Select biallelic sites only? By default set to TRUE, to allow downstream analyses with SambaR. But, set to FALSE to generate input for VCF_calcdist.sh.
combinevcf3=FALSE		## Combine subset vcf files with snps. But faster option is to run missfilter on subset vcf files and combine with combinevcf4 step. 
						# Final output for mtDNA and Y-chromosomal data. Use this file as input for VCF_haploid2diploid.
						# For autosomal data, use this file as input for VCF_calcdist calculations, as well as input for easySFS. (But make sure to know the total number of sites of the vcf-file prior to selecting variants.)	

# SNP FILTERING (OPTIONAL):
missfilter=FALSE		## Use minalleles flag (next line) to set missfilter. This is a repetition of min_alleles flag in globalfilter step. 
minalleles=32			# recommendation: in the range of 1.6 - 1.95 * nrsamples in case of diploid data, and in the range of 0.8 - 0.975 * nrsamples in case of haploid data.   
combinevcf4=FALSE		## Combine subset vcf files with filtered snps.	

# SNP THINNING (OPTIONAL):
THININPUT=${PREFIX}.mysnps.thin${thinfactor}.missfilter.vcf.gz		# Specify input vcf-file for snp thinning. Either output of combinevcf3 or combinevcf4. 
thinsnps=FALSE			## Use the 'thinsnpsbp' flag (next line) to adjust the thinning parameter (i.e. minimum distance between snps in bp). Default is 1 SNP per 50Kb
thinsnpbp=50000			#

# FILE CONVERSION:
FINALVCF=${PREFIX}.mysnps.thin0_50000.missfilter.vcf.gz				# Specify input vcf-file for file conversion and depth calculation. Either output of combinevcf3 or combinevcf4 or thinsnps (default)
calcsnpdepth=FALSE		## Use FINALVCF flag to specify input.
convert2ped=FALSE		## run PLINK to convert vcf to PED/MAP and subsequently to RAW/BIM
convert2beagle=FALSE		## Note: not tested yet. Use FINALVCF to specify input.	

#################################################################################
# User shouldn't change anything from here.



if [[ "$createbed" = TRUE ]]
	then
	echo "Start creating bed files"
	${SAMTOOLS} faidx ${REFERENCE}                                                  # find out total number of bp in your genome
	cut -f1-2 ${REFERENCE}.fai > contigs.bed
	awk '{ total += $2 } END { print total }' contigs.bed > nrsites.txt             # I find: 2607875777. We are going to divide this over NRSETS equal batches (by generating bedfiles, which serve as input for samtools mpileup)
	NRSITES=$(cat nrsites.txt)
	cut -f2 contigs.bed > column2.txt
	perl -lne 'print $sum+=$_' column2.txt > cum.txt
	paste -d '\t' contigs.bed cum.txt > contigs.cum.bed

	echo "Subdivisions:"
	seq 1 $NRSETS > mynumbers.txt
	for digit in $(cat mynumbers.txt)
		do
		num=$(($digit-1))
		part=$((${NRSITES}/${NRSETS}))
		var1=$((${num} * ${part}))
		var2=$((${digit} * ${part}))
		echo $digit
		echo $var1
		echo $var2
		awk -v mystart="${var1}" -v myend="${var2}" '{ if ( $3 >= mystart && $3 <= myend ) print > "mybed.tmp.txt" }' contigs.cum.bed
		if [ -f "mybed.tmp.txt" ]
			then
			cut -f1 mybed.tmp.txt > contignames.txt
			cut -f2 mybed.tmp.txt > contiglengths.txt
			sed -i 's/^/0___/' contiglengths.txt
			sed -i 's/___/\t/g' contiglengths.txt
			paste -d '\t' contignames.txt contiglengths.txt > mybed.tmp.txt
			awk '{ if ($2 == "0") $2=1; print $0 }' mybed.tmp.txt | sed 's/ /\t/g' > mybed${digit}.txt      # bcftools expects 1 based reference system
			rm mybed.tmp.txt
		fi
		done
		rm mynumbers.txt nrsites.txt contig* cum.txt column2.txt

	echo "Creating overview of number of bp per subset..."
	ls -1 mybed*txt > bedfiles.nrsites.tmp1.txt
	if [ -f "bedfiles.nrsites.tmp2.txt" ]; then rm bedfiles.nrsites.tmp2.txt; fi
	touch bedfiles.nrsites.tmp2.txt
	for bedfile in mybed*txt
		do
		nrsites=$(awk '{sum+=$3;} END{print sum;}' $bedfile)
		echo $nrsites >> bedfiles.nrsites.tmp2.txt
		done
	paste bedfiles.nrsites.tmp1.txt bedfiles.nrsites.tmp2.txt > bedfiles.nrsites.txt
	rm bedfiles.nrsites.tmp1.txt bedfiles.nrsites.tmp2.txt

	echo "Finished creating bed files."
	echo "Note that the number of bed files might be lower than specified, because this script does not chop up chromosomes (and hence number of chromosomes may set an upper boundary)."
	echo "You can observe the file 'bedfiles.nrsites.txt' to see if sites are relatively equally distributed over subsets."
	echo "Often the last file (containing unplaced scaffolds) contains considerable more sites than the other files."
	echo "It might be worthwhile to split this file into multiple files otherwise this file may cause a delay."
	echo "Also check whether there are no empty mybed.txt files. If so, remove these files, because they will causes errors when running the next step (runpipeline)."
fi

if [[ "$runpipeline" = TRUE ]]
	then
	echo "Starting pipeline of bcftools mpileup, call, norm and filter (all at once)..."
	# bcftools mpileup uses 100%, whereas call, norm and filter use around 30%. So in total, per bed file, around 200% (i.e., 2 cores). 
	# For example: if you have 25 bed files, this command will use 50% cores in total.
	# Note: max-depth per sample is set to 20000, just in case working with with mtDNA.
	# the option -C50 is for downgrading mapping quality for reads with excessive mismatches. It is recommended when using bam-files generated by bwa 
	for mybedfile in mybed*.txt
	do
	echo ${mybedfile}
	if [[ "$setploidy" = TRUE ]]
		then
		${BCFTOOLS} mpileup -A --max-depth $maxsampledepth --min-MQ 20 -min-BQ 15 -C50 -a "DP,AD" -R ${mybedfile} --output-type u -f ${REFERENCE} --bam-list ${BAMFILES} | 
		${BCFTOOLS} call -a GQ -m --group-samples $POPFILE --output-type z --ploidy-file $PLOIDYFILE --samples-file $SEXFILE | 					# this line is affected by setploidy flag 
		${BCFTOOLS} norm --check-ref w --fasta-ref ${REFERENCE} -O z |
		${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP<${maskdepth}" -O z -o masked_${PREFIX}.${mybedfile}.vcf.gz &
		else
		${BCFTOOLS} mpileup -A --max-depth $maxsampledepth --min-MQ 20 -min-BQ 15 -C50 -a "DP,AD" -R ${mybedfile} --output-type u -f ${REFERENCE} --bam-list ${BAMFILES} | 
		${BCFTOOLS} call -a GQ -m --group-samples $POPFILE --output-type z --ploidy ${PLOIDY} | 												# this line is affected by setploidy flag
		${BCFTOOLS} norm --check-ref w --fasta-ref ${REFERENCE} -O z |
		${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP<${maskdepth}" -O z -o masked_${PREFIX}.${mybedfile}.vcf.gz &
	fi
	done
	wait
	echo "Finished bcftools mpileup/call/norm/filter pipeline."
	echo "You can directly proceed with or globalfilter step and skip all steps in between."
	echo "On the other hand, to know which min and max depth filter thresholds to apply, it would be helpful run the calcdepth filter." 
fi

if [[ "$combinevcf" = TRUE ]]
	then
	echo "Creating sorted list of vcf files..."
	ls masked_${PREFIX}.mybed*.txt.vcf.gz > myvcfsubset.files.txt
	cut -f2 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
	paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
	rm myvcfsubset.files.txt myvcfsubset.numbers.txt
	echo "Combining (i.e. concatenating) vcf files..."
	${BCFTOOLS} concat --threads 35 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.masked.vcf.gz
	wait
	echo "Finished combining vcf files. Output is stored in file 'PREFIX.masked.vcf.gz' (previously 'PREFIX.combined.vcf.gz')."
fi

if [[ "$calcdepth" = TRUE ]]
	then
	echo "Extracting depth information using vcftools..."
	${VCFTOOLS} --gzvcf masked_${PREFIX}.${myregion}.vcf.gz --depth --out sampledepth &
	#
	echo "Extracting sample names using bcftools query..."
	${BCFTOOLS} query --list-samples masked_${PREFIX}.${myregion}.vcf.gz > myinfo.samples.txt &
	echo "Thinning data using vcftools..."	
	${VCFTOOLS} -c --thin ${depththinbp} --gzvcf masked_${PREFIX}.${myregion}.vcf.gz --recode --recode-INFO-all | gzip > ${PREFIX}.${myregion}.thin${depththinbp}.vcf.gz
	echo "Extracting depth information from thinned dataset using bcftools query..."
	${BCFTOOLS} query -f '%DP[\t%DP]\n' ${PREFIX}.${myregion}.thin${depththinbp}.vcf.gz > sampleinfo.DP.txt
	${BCFTOOLS} query -f '%DP[\t%GQ]\n' ${PREFIX}.${myregion}.thin${depththinbp}.vcf.gz > sampleinfo.GQ.txt
	${BCFTOOLS} query -f '%POS\n' ${PREFIX}.${myregion}.thin${depththinbp}.vcf.gz > sampleinfo.pos.txt
	wait
	echo "Depth information (mean per sample) stored in file called 'sampledepth.depth.out'."
	echo "Depth information (subset of sites) stored in file called 'sampledepth.info.txt'."
	echo "Use the BAM2VCF_plotdepth.inR.txt' script to visualise the data, which will help you to select the correct mindepth and maxdepth thresholds for the globalfilter step."
	echo "Choose mindepth and maxdepth such that you select the main peak only, and exclude any satellite peaks (if present)."
fi

if [[ "$globalfilter" = TRUE ]]
	then
	# three options for filter (e.g. read depth):
	# - global soft filter:			remove a site (entire line) if overall read depth (all samples combined) falls outside a specified range
	# - global hard filter:			remove a site (entire line) if one or more samples fall outside a specified range
	# - mask filter:				set a genotype to missing (./.) if read depth for this particular site and for this particular individual falls outside a specified range, and then filter on number of missing data points  		
	# Note: this script only applies a global soft filter (this step, globalfilter step) and a mask filter (already applied in the runpipeline step).
	#	
	# Read depth (DP):
	# Observe output of getstats step to make an informed decision about read depth filter settings, customized to your dataset.
	# Rough guidelines: 
	# Minimum depth: around 6 times nrsamples. 
	# Maximum depth: maximum 1.5x mean depth (higher depth could indicate a paralogous locus, or telomeres and centromeres).
	#
	# Site quality (6th column):
	# For now, we want to find a dataset of reliable sites, both monomorphic and polymorphic, and we do not care about the ratio between reference and alternative alleles. 
	# Therefore, we do not filter on QUAL at this step. Filtering on QUAL score, might in fact bias downstream SFS calculations (as sites with low minor allele counts, might be filtered out).
	# 16-10-2024: quality filter (bcftools filter -i "QUAL>=30") no longer included. 06-02-2025: now added as option.
	#
	# SnpGap:
	# No snpGap filter, because I didn't find evidence for elevated levels of heterozygosity close to indels.
	#
	# Missingness per site filter:
	# bcftools view -i "AN>=${min_alleles}"
	# For example, in case the dataset contains 100 diploid individuals, and in case you want to retain only sites with no missing data at all (which I would not recommend), then it would: bcftools view -i "AN>=200"
	# For that extreme example, you could also used: bcftools view -e 'GT[*]="."'
	#
	echo "Filtering vcf-file using a global filter..."
	if [ -f masked_${PREFIX}.${myregion}.vcf.gz ]; 
		then
		echo "Running analysis per subset..."
		for mybedfile in mybed*.txt
			do
			echo ${mybedfile}
			if [[ "$removeindels" = TRUE ]]
				then
				echo "Also removing indels..."
				$BCFTOOLS view --threads 3 -i "INFO/DP>=${mindepth} && INFO/DP<=${maxdepth} && AN>=${min_alleles} && QUAL>=${site_quality}" masked_${PREFIX}.${mybedfile}.vcf.gz -O z -o ${PREFIX}.globalfilter.thin0.${mybedfile}.vcf.gz --exclude-types indels &
				else
				$BCFTOOLS view --threads 3 -i "INFO/DP>=${mindepth} && INFO/DP<=${maxdepth} && AN>=${min_alleles} && QUAL>=${site_quality}" masked_${PREFIX}.${mybedfile}.vcf.gz -O z -o ${PREFIX}.globalfilter.thin0.${mybedfile}.vcf.gz &
			fi
			done
		wait
		echo "Finished global filter. Output is stored in the files 'PREFIX.globalfilter.thin0.mybed1.txt.vcf.gz' etc."
		echo "Proceed either with combining vcf files (using combinevcf2 flag) or alternatively with SNP calling on subsets (selectvariants flag)."
		else
		if [ -f ${PREFIX}.masked.vcf.gz ];
			then
			echo "Running analysis for combined vcf file."
			if [[ "$removeindels" = TRUE ]]
				then
				$BCFTOOLS view --threads 3 -i "INFO/DP>=${mindepth} && INFO/DP<=${maxdepth} && AN>=${min_alleles} && QUAL>=${site_quality}" ${PREFIX}.masked.vcf.gz -O z -o ${PREFIX}.globalfilter.thin0.vcf.gz --exclude-types indels
				else
				$BCFTOOLS view --threads 3 -i "INFO/DP>=${mindepth} && INFO/DP<=${maxdepth} && AN>=${min_alleles} && QUAL>=${site_quality}" ${PREFIX}.masked.vcf.gz -O z -o ${PREFIX}.globalfilter.thin0.vcf.gz
			fi
			wait
			echo "Finished global filter. Output is stored in the file 'PREFIX.globalfilter.thin0.vcf.gz'."
			else
			echo "ERROR: No input files detected. Cannot run analysis. (Perhaps specify another bed file to myregion flag?)"
		fi
	fi
fi  

# Optional step (not strictly needed as next step can also run on subset vcf files.)
# However, needed as input for Darwindow (genome-wide heterozygosity) and also for VCF_calcdist.sh script. 
if [[ "$combinevcf2" = TRUE ]]
	then
	echo "Creating sorted list of vcf files..."
	ls ${PREFIX}.globalfilter.thin0.mybed*.txt.vcf.gz > myvcfsubset.files.txt
	cut -f4 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
	paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
	rm myvcfsubset.files.txt myvcfsubset.numbers.txt
	echo "Combining (i.e. concatenating) vcf files..."
	${BCFTOOLS} concat --threads 40 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.globalfilter.thin0.vcf.gz
	wait
	echo "Finished combining vcf files. Output is stored in file called 'PREFIX.globalfilter.thin0.vcf.gz'."
	echo "This output file can be used as input for ROH-analyses with Darwindow."
fi

if [[ "$thinallsites" = TRUE ]]
	then
	echo "Thinning vcf file..."
	# This step can be used to shrink the dataset, so that it can be imported into R (for example).
	# I use thinning as replacement for LD filtering.
	# This is faster, less computational expensive, and it is anyway not possible to apply and LD-filter to monomorphic sites.
	# But, as I see it, even for polymorphic sites, thinning is more to the point than LD-filter: you only want to filter out SNP pairs which are in physical linkage, not linkage due to other factors such as selection.
	if [ -f ${PREFIX}.globalfilter.thin0.${myregion}.vcf.gz ];
		then
		echo "Running analysis per subset..."
		for mybedfile in mybed*.txt
			do
			echo ${mybedfile}
			${VCFTOOLS} --thin $thinallbp --gzvcf ${PREFIX}.globalfilter.thin0.${mybedfile}.vcf.gz -c --recode --recode-INFO-all | gzip > ${PREFIX}.globalfilter.thin${thinallbp}.${mybedfile}.vcf.gz &
			done
			wait
			echo "Finished thinning. Output is stored in files called 'PREFIX.globalfilter.thinX.mybed1.txt.vcf.gz' etc."
		else
		${VCFTOOLS} --thin $thinallbp --gzvcf ${PREFIX}.globalfilter.thin0.vcf.gz -c --recode --recode-INFO-all | gzip > ${PREFIX}.globalfilter.thin${thinallbp}.vcf.gz &
		wait
		echo "Finished thinning. Output is stored in the file 'PREFIX.globalfilter.thinX.vcf.gz'."
	fi
fi

if [[ "$combinethin" = TRUE ]]
	then
	if [ -f ${PREFIX}.globalfilter.thin${thinallbp}.${myregion}.vcf.gz ]
		then
		echo "Creating sorted list of vcf files..."
		ls ${PREFIX}.globalfilter.thin${thinallbp}.mybed*txt.vcf.gz > myvcfsubset.files.txt
		cut -f4 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
		paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
		rm myvcfsubset.files.txt myvcfsubset.numbers.txt
		echo "Combining (i.e. concatenating) vcf files..."
		${BCFTOOLS} concat --threads 35 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.globalfilter.thin${thinallbp}.vcf.gz
		wait
		echo "Finished combining vcf files. Output is stored in file called 'PREFIX.globalfilter.thinX.vcf.gz'."
		else
		echo "ERROR: No input files (e.g. PREFIX.globalfilter.thinX.mybed1.txt.vcf.gz) detected. Cannot combine."
	fi
fi

if [[ "$countsites" = TRUE ]]
	then
	for myfile in ${PREFIX}.globalfilter.thin${thinallbp}.mybed*txt.vcf.gz
        do
        ls $myfile
        ${BCFTOOLS} stats $myfile > ${myfile}.stats.txt &
        done
	wait
	echo "Calculating total sum..."
	grep 'number of records:' ${PREFIX}.globalfilter.thin${thinallbp}.mybed*txt.vcf.gz.stats.txt | cut -f4 | awk '{total += $1}END{print total}'
fi

if [[ "$selectvariants" = TRUE ]]
	then
	echo "Selecting polymorphic sites..."
	# 16-10-2024: quality filter (bcftools filter -i "QUAL>=30") no longer included
	if [ -f ${PREFIX}.globalfilter.thin${thinfactor}.${myregion}.vcf.gz ]; 
		then
		echo "Running analysis per subset..."
		for mybedfile in mybed*.txt
			do
			echo ${mybedfile}
			if [[ "$biallelic" = TRUE ]]
				then
				echo "Selecting biallelic sites only (default setting)..."	
				$BCFTOOLS view --threads 3 --exclude-types indels -O z ${PREFIX}.globalfilter.thin${thinfactor}.${mybedfile}.vcf.gz -o ${PREFIX}.mysnps.thin${thinfactor}.${mybedfile}.vcf.gz --min-alleles 2 --max-alleles 2 &
				else
				echo "Selecting biallelic AND multiallelic sites..."
				$BCFTOOLS view --threads 3 --exclude-types indels -O z ${PREFIX}.globalfilter.thin${thinfactor}.${mybedfile}.vcf.gz -o ${PREFIX}.mysnps.thin${thinfactor}.${mybedfile}.vcf.gz --min-alleles 2 &
			fi
			done
		wait
		echo "Finished selecting biallelic snps. Output is stored in the files 'PREFIX.mysnps.thinX.mybed1.txt.vcf.gz' etc."
		echo "Proceed with combining these files using combinevcf3 flag."
		else
		if [ -f ${PREFIX}.allsites.globalfilter.thin${thinfactor}.vcf.gz ];
			then
			echo "Selecting biallelic SNPs from combined vcf file."		
			if [[ "$biallelic" = TRUE ]]
				then
				echo "Selecting biallelic sites only (default setting)..."	
				$BCFTOOLS view --threads 10 --exclude-types indels -O z ${PREFIX}.allsites.globalfilter.thin${thinfactor}.vcf.gz -o ${PREFIX}.mysnps.thin${thinfactor}.vcf.gz --min-alleles 2 --max-alleles 2 
				else
				$BCFTOOLS view --threads 10 --exclude-types indels -O z ${PREFIX}.allsites.globalfilter.thin${thinfactor}.vcf.gz -o ${PREFIX}.mysnps.thin${thinfactor}.vcf.gz --min-alleles 2 
			fi
			wait
			echo "Finished selecting variant sites. Output is stored in the file 'PREFIX.mysnps.thinX.vcf.gz'."
			else
			echo "ERROR: No input files detected. Cannot run analysis."
		fi
	fi	
fi

if [[ "$combinevcf3" = TRUE ]]
	then
	if [ -f ${PREFIX}.mysnps.thin${thinfactor}.${myregion}.vcf.gz ]
		then
		echo "Creating sorted list of vcf files..."
		ls ${PREFIX}.mysnps.thin${thinfactor}.mybed*.txt.vcf.gz > myvcfsubset.files.txt
		cut -f4 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
		paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
		rm myvcfsubset.files.txt myvcfsubset.numbers.txt
		echo "Combining (i.e. concatenating) vcf files..."
		${BCFTOOLS} concat --threads 35 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.mysnps.thin${thinfactor}.vcf.gz
		wait
		echo "Finished combining vcf files. Output is stored in file called 'PREFIX.mysnps.thinX.vcf.gz'."
		echo "You can proceed with the missfilter step."
		else
		echo "ERROR: No input files (e.g. PREFIX.mysnps.mybed1.txt.vcf.gz) detected. Cannot combine."
	fi
fi

if [[ "$missfilter" = TRUE ]]
	then
	# FILTER ON MISSING DATA:
	# To next filter on missing data replace 1.9*NRSAMPLES with desired (rounded) number. Uses up to 5 (?) cores.
	# This step is mandatory even when the missfilter has already been applied during the globalfilter step.
	echo "Filtering on missing data..."
	if [ -f ${PREFIX}.mysnps.thin${thinfactor}.${myregion}.vcf.gz ];
		then
		echo "Running analysis per subset..."
		for mybedfile in mybed*.txt
			do
			echo ${mybedfile}	
			$BCFTOOLS view --threads 3 -i "AN>=${minalleles}" -O z ${PREFIX}.mysnps.thin${thinfactor}.${mybedfile}.vcf.gz -o ${PREFIX}.mysnps.thin${thinfactor}.missfilter.${mybedfile}.vcf.gz &
			done
		wait
		echo "Finished filtering on missing data. Output is stored in the files 'PREFIX.mysnps.thinX.missfilter.mybed1.txt.vcf.gz'."
		else
		echo "Running analysis on combined vcf-file..."
		$BCFTOOLS view --threads 10 -i "AN>=${minalleles}" -O z ${PREFIX}.mysnps.thin${thinfactor}.vcf.gz -o ${PREFIX}.mysnps.thin${thinfactor}.missfilter.vcf.gz
		echo "Finished filtering on missing data. Output is stored in the files 'PREFIX.mysnps.thinX.missfilter.vcf.gz' etc."
	fi
fi

if [[ "$combinevcf4" = TRUE ]]
    then
    if [ -f ${PREFIX}.mysnps.thin${thinfactor}.missfilter.${myregion}.vcf.gz ]
        then
		echo "Creating sorted list of vcf files..."
        ls ${PREFIX}.mysnps.thin${thinfactor}.missfilter.mybed*.txt.vcf.gz > myvcfsubset.files.txt
        cut -f5 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
        paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
        rm myvcfsubset.files.txt myvcfsubset.numbers.txt
        echo "Combining (i.e. concatenating) vcf files..."
		${BCFTOOLS} concat --threads 35 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.mysnps.thin${thinfactor}.missfilter.vcf.gz
        wait
        echo "Finished combining vcf files. Output is stored in file called 'PREFIX.mysnps.thinX.missfilter.vcf.gz'."
        echo "You can proceed with the thinning step."
        else
        echo "ERROR: No input files (e.g. PREFIX.mysnps.thinX.missfilter.mybed1.txt.vcf.gz) detected. Cannot combine."
	fi
fi

if [[ "$thinsnps" = TRUE ]]
	then
	# This mandatory step can be used to shrink the SNP dataset, for example for practical reasons, so that it can be imported into R (for example).
	# I use thinning as replacement for LD filtering.
   	# This is faster, less computational expensive, and as I see it more to the point: you only want to filter out SNP pairs which are in physical linkage, not linkage due to other factors such as selection.
    echo "Thinning vcf file..."
    ${VCFTOOLS} --thin $thinsnpbp --gzvcf ${THININPUT} -c --recode --recode-INFO-all | gzip > ${PREFIX}.mysnps.thin${thinfactor}_${thinsnpbp}.missfilter.vcf.gz &
    wait
    echo "Finished thinning. Output is stored in the file 'PREFIX.mysnps.thinX_X.missfilter.vcf.gz'."
fi

if [[ "$calcsnpdepth" = TRUE ]]
    then
   	# Note: this works only if -a "DP" has been specified during bcftools mpileup run
    	# If not, DP info field will be absent from sample genotype column (see 9th column, not 8th column, of vcf file)
    	# If absent, output depth files will contain nan-values only.
    	if [ -f $FINALVCF ]
		then
		echo "Calculating depth per sample..."
    		${VCFTOOLS} --gzvcf ${FINALVCF} --depth --out ${PREFIX}.mysnps &
    		echo "Calculating depth per site..."
    		${VCFTOOLS} --gzvcf ${FINALVCF} --site-mean-depth --out ${PREFIX}.mysnps &
    		wait
    		echo "Finished snp depth calculations. Results stored in files 'PREFIX.mysnps.idepth' and 'PREFIX.mysnps.ldepth.mean'."
		else
		echo "ERROR: input file not found."
	fi
fi

if [[ "$convert2ped" = TRUE ]]
    then
	if [ -f $FINALVCF ]
		then
    	echo "Converting vcf to ped/map..."
    	${VCFTOOLS} --gzvcf ${FINALVCF} --plink --out ${PREFIX}.mysnps
    	wait
    	# edit first column of map file:
    	cut -f2 ${PREFIX}.mysnps.map | cut -f1 -d ':' > mycontigs.txt && cut -f2,3,4 ${PREFIX}.mysnps.map > mymap.txt && paste mycontigs.txt mymap.txt > ${PREFIX}.mysnps.map && rm mycontigs.txt mymap.txt
    	## alternatively (to not having to edit first column of map-file): 
		# zgrep -v '#' inputfile.vcf | cut -f1 | sort | uniq > mychroms.txt
		# ${VCFTOOLS} --gzvcf ${FINALVCF} --plink --out ${PREFIX}.mysnps --chrom-map mychroms.txt
		wait
    	echo "Converting ped/map to raw/bim..."
    	${PLINK} --file ${PREFIX}.mysnps --allow-extra-chr --chr-set 95 --make-bed --recode A --out ${PREFIX}.mysnps
    	echo "Finished converting vcf file to ped/map and raw/bim format."
		echo "You are ready to run population-genetic analyses (for example using SAMBAR)."
    	# an alternative is to use plink: plink --vcf ${FINALVCF} --recode --out mysnps 
    	# /home/mdejong/software/plinkv2020-09-21/plink --vcf ${PREFIX}.mysnps.thin100.vcf.gz --recode A --make-bed --out mysnps
		else
		echo "ERROR: input file not found."
	fi
fi

if [[ "$convert2beagle" = TRUE ]]
    then
	if [ -f $FINALVCF ]
        then
		# Note: extracts from input file the biallelic sites only
    		# By default, only considers first 25 scaffolds. Edit head -25 to contain more or less.
    		grep -m1 -B2000000 '#CHROM' ${FINALVCF} | grep '##contig' | cut -d ',' -f1 | cut -d '=' -f3 | head -25 > mysnps.scaffolds.txt
    		for mychrom in $(cat mysnps.scaffolds.txt)
    			do
    			${VCFTOOLS} --gzvcf ${FINALVCF} --BEAGLE-PL --chr $mychrom --out ${PREFIX}.mysnps.${mychrom}
    			done
		wait
    	echo "Finished converting vcf file :-)"
		else
		echo "ERROR: input file not found."
	fi
fi






############ BCFTOOLS OPTIONS ###########



## BCFTOOLS MPILEUP:
# base quality score of Illumina 1.8+ ranges from 0 to 41, and are denoted by, from low to high: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI. If using threshold of 13, any base with value on left hand side from / is deleted. 
# Q: min-BQ: minimum base quality (default is 13, which roughly corresponds to a probability of 0.05 that base call is incorrect, as given by the formula -10*log10(0.05))
# q: min-MQ: minimum mapping quality (default is 0)
# C: adjust-MQ: 	coefficient for downgrading mapping quality for reads containing excessive mismatches. The new mapping quality is about sqrt((INT-q)/INT)*INT, with q being 'phred-scaled probability of being generated from the mapped position'.
#					Given I don't fully understand this option, and given that later on I filter on alignment score anywhere, I prefer not to use this option, either by remove the part -C50 or by setting to -C0 (which is default).	

# by default only PL in sample info columns format
# with the -a flag (annotate flag) we add AD (allele depth) and DP (total depth), so that during bcftools call this information can be included in the sample genotype columns

# What about -B or --no-BAQ?
# For low depth data (around 15x depth):
# -B or --no-BAQ:	do NOT set this flag!! 
# Specifying this option will disable probabilistic realignment for the computation of base alignment quality (BAQ), which greatly helps to reduce false SNPs caused by misalignments.
# BAQ is the Phred-scaled probability of a read base being misaligned.
# m: min-ireads: minimum number gapped reads needed to infer an indel. By default: 1. This threshold could be set a bit higher, e.g. 2 or 3.
# For high depth data (around 60x depth): 
# DO set --B but use a higher min-BQ threshold.
# The reason is that the BAQ calculations have been written for low depth data, and the underlying algorithm is not suitable for high depth data.



## BCFTOOLS NORM:
# bcftools norm is for multi nucleotide variant (MNV) left alignment and normalization. 
# bcftools norm also has the option to split multi-allelic into bi-allelic, but this option is not activated by this script, because having multiple lines for a single site would screw up Darwindow calculations.
# The bcftools norm command might return to the screen many lines starting with 'NON_ACGTN_REF'
# If using the flag '--check-ref e' with e for error rather than w for warning, bcftools would abort. 



# BCFTOOLS CALL:
# Better not to multi-thread per file, because depending on the number of subsets you might overload the system (e.g 25x3=75 cores)
# choose c (consensus) or m (multicaller) mode. Consensus is the original model, but multicaller mode (which considers more than 2 alleles per samples?) is now preferred.   
# -p, --pval-threshold float: if in mode -c, accept variant if P(ref|D) < float. In words: assume alternative allele if probability of reference allele given the available data is below value
# Do not use -v (--variants-only) flag, otherwise information on monomorphic sites will get lost, which makes it impossible to calculate measures such as He, pi and Dxy. 
#
# --annotate (-a) flag can add extra fields
# Note, the --annotate flag has replaced the --format-fields flag:
# -- format-fields (-f) can add GQ,GP fields (If you try to add other terms, you will receive an error)
#
# By default, genotype format of output vcf file:
# GT:PL
# If flag -a "DP,PL,AD" has been specified during mpileup run, then automatically it becomes:
# GT:DP:AD			# if ALT is absent		e.g. 0/0:16:16
# GT:PL:DP:AD		# if ALT is present		e.g. 0/0:0,12,108:4:4,0
#
# We add Genotype Quality (which can be used for filtering by vcftools). The genotype format becomes: 
# GT:DP:AD			# if ALT is absent		e.g. 0/0:16:16
# GT:PL:DP:AD:GQ	# if ALT is present		e.g. 0/0:0,12,108:4:4,0:127 



# BCFTOOLS VIEW (used for masking):

# If same settings for all samples:
# OPTION A:
# $VCFTOOLS --gzvcf $myvcffile -c --minDP 3 --recode --recode-INFO-all | gzip > masked_${myvcffile} &
# OPTION B (used by this script):
# $BCFTOOLS filter --threads 1 --set-GTs . -e "FMT/DP<${maskdepth}" $myvcffile -O z -o masked_${myvcffile} &
# --set-GTs or --S flag:	We set failed genotypes to missing (.). (Other option would be to set to reference: --set-GTs 0) 
# Note: if using -S option, and if wanting to include multiple criteria, denote OR by |, not by ||. For example: -e 'FMT/DP<5 | FMT/GQ<10'
#
# Which read depth (DP) and/or genotype quality (GQ) filter should you choose?
# Scroll down for more information about the meaning and the calculation of the GQ score.
# According to online tutorials, if the groups defined to the flag --group-samples contain reasonable numbers of samples, we can safely keep genotypes with read depth as low as 3. 
# The justification is that bcftools calls genotypes across all samples belong to the same group simultaneously, and includes the information from all reads from all individuals to adjust the genotype quality. 
# A call with DP=3 may thus have a high GQ and can be confidently kept.
# To account for groups containing 1 sample only (used by this script), one may prefer to use a higher miminum read depth.
#
# If settings needs to vary across samples:  
# If you want to use different thresholds depending on the sample, you have to first subset the input vcf files, and then later recombine.
# Here assuming that you want to divide your samples in 3 different sets (which each unique mask settings):
# ${BCFTOOLS} view --threads 1 --samples-file 2samples.txt $myvcffile | ${BCFTOOLS} filter --threads 1 --set-GTs . -e 'FMT/DP>20 | FMT/DP<4' -O z -o masked_${myvcffile}_1 &
# ${BCFTOOLS} view --threads 1 --samples-file 2samples.txt $myvcffile | ${BCFTOOLS} filter --threads 1 --set-GTs . -e 'FMT/DP>20 | FMT/DP<6' -O z -o masked_${myvcffile}_2 &
# ${BCFTOOLS} view --threads 1 --samples-file 2samples.txt $myvcffile | ${BCFTOOLS} filter --threads 1 --set-GTs . -e 'FMT/DP>20 | FMT/DP<8' -O z -o masked_${myvcffile}_3 &






############### FURTHER EXPLANATIONS ##########

# SPLITTING GENOME IN EQUAL SECTIONS

# You might think an easy way to split the genome into regions is simply by dividing the number of contigs evenly.
# However, some contigs are way bigger than others. If the big contigs are clumped, you can get a very uneven distribution.
# For example, this is the distribution of sites I got when dividing the number of contigs by 4:
# 2,386,470,152
# 21,398,024
# 31,723,389
# 168,284,212
# With this distribution, the first mpileuploop will run forever, and the others will be finished fairly quickly.
# So this way you don't gain much time. Therefore we have to divide by number of sites rather than by contigs.

# the awk loop above execute these commands:
# Say that you specified to divide the genome in 16 regions:
# awk '{ if ($4 >= 0*2607875777/16 && $4 <= 1*2607875777/16 ) print > "mybed1.txt" }' contigs.cum.bed
# awk '{ if ($4 >= 1*2607875777/16 && $4 <= 2*2607875777/16 ) print > "mybed2.txt" }' contigs.cum.bed
# etc
# until
# awk '{ if ($4 >= 15*2607875777/16 && $4 <= 16*2607875777/16 ) print > "mybed16.txt" }' contigs.cum.bed



# DIFFERENT TYPE OF QUALITY SCORES/FILTERS

# Along the way from fastq via bam to vcf, there are several quality scores, and it is easy to get confused between them.
# Here an overview of the most important ones:

# - QUAL (fastq) 
#	name:		base quality score
#	where:		every 4th line in fastq, 11th column in SAM/BAM file
#	meaning:	QUAL, is the sequencing quality, which can be platform biased, e.g. Ion seemed to have lower QUAL compared to Illumina.
# 	formula:	?

# - MQ (bam) 
#	name:		mapping quality 
#	where:		5th column in SAM/BAM file; mean values per site are given in MQ-field in 8th column (INFO) of VCF  
#	meaning:	typically an indication of how unique the region's sequence is: the higher the MQ, the more unique the sequence, and to less regions the read could have mapped 
#	formula:	?

# - GQ (bcf/vcf)
#	name:		genotype quality 
#	where:		GQ field of individual columns (10th and higher), as indicated by format column (9th column)
#	meaning:	difference in phred-scaled genotype likelihood (PL) between the lowest and second lowest genotype likelihood
#	formula:	scroll down for more info
#	filter:		I decided to mask genotypes based on read depth rather than on GQ (scroll down for justification)
#	command:	vcftools --gzvcf inputvcf.gz --minDP 6 [--minGQ ?]--recode --recode-INFO-all | gzip > masked_${myvcffile} & 

# -	FQ (vcf)
#	name:		consensus quality
#	where:		FQ-field in 8th column (INFO) of VCF
#	meaning:	If positive, FQ equals the phred-scaled probability of there being two or more different alleles.
#				If negative, FQ equals the minus phred-scaled probability of all chromosomes being identical.
# 				Notably, given one sample, FQ is positive at hets and negative at homs.
# 	usage:		generated by bcftools call -c (so consensus mode), not -m (multiallelic mode)
#				used by vcfutils vcf2fq to create quality scores in output fastq files  

# -	QUAL (vcf)
#	name:		site quality
#	where:		6th column in VCF file
#	meaning:	probability that alternative allele is present (in case of monomorphic site) or absent (in case of polymorphic site). 
# 	formula:	Let P be the probability of the sample having a variant allele. If ALT!='.', QUAL equals -10log10(1-P). If ALT=='.', QUAL equals -10log10(P). I expect it depends on DP4 (number of reference/alternative alleles). 
#	filter:		for snps, filter out sites in which quality is below 30, i.e. if probability is higher than 0.001 (1/1000) that site is incorrectly called polymorphic
#	command:	bcftools filter 'QUAL>=30' input.vcf.gz

# Some additional information regarding site quality (6th column of VCF-file):
# QUAL, 6th column, is the probability that alternative allele is present (in case of monomorphic site) or absent (in case of polymorphic site).
# Consider the following DP4 scores (which indicates: depth for REF_forward, REF_reverse, ALT_forward, ALT_reverse; in other words number of forward and reverse reads with REF and ALT allele). 
# If DP4 is 100,100,0,0 then site is likely called monomorphic with a high quality score.
# If DP4 is 97,97,3,3 then site is likely called monomorphic with a low quality score.
# If DP4 is 50,50,50,50 then site is likely called polymorphic with a high quality score.
# This measure is useful for the purpose of extracting highly informative SNPs, but may also introduce bias because you are indirectly filtering out sites with low minor allele counts.




### THE CALCULATION OF GENOTYPE LIKELIHOODS

# Genotype likelihoods are the probability of a genotype given the observed data.
# Likelihoods are defined as:
# Pr(H|O) means: probability of the hypothesis (H) given the observed data (O)
# For genotyping this becomes:
# Pr(G|D) means: probability of a genotype (e.g 0/0, 0/1, 1/1) given the observed sequence data (D)

# GATK uses the following formula (bcftools mpileup as well?), based on Bayes Rule:
# Pr(G|D) =	(Pr(G)*Pr(D|G))/(sum(P(G_i)*P(D|G_i))) 
# If using a flat prior probability (which is default and which means that at the start each genotype is considered equally likely), then the formula simplifies to:
# Pr(G|D) =	(Pr(D|G))/(sum(P(D|G_i))) 

# The numeration (top part of the division, Pr(D|G), basically means: 
# probability of obtaining the observed sequence data (more precisely: allelic depths) given the hypothesized genotype
# The denominator is the sum of all genotype probabilities (the i in the formula is meant to denote each different genotype)

# For example, say that you have 10 reads, of which 2 are A (as reference) and 8 are T (alternative). Then:
# P(D|G=0/0) = Pr(2A+8T|A/A)
# P(D|G=0/1) = Pr(2A+8T|A/T)
# P(D|G=1/1) = Pr(2A+8T|T/T)
# Say for example that the error rate is 0.1, it is not unthinkable that you would observe some T's in the sequence data even though the individual is in reality homozygous alternative.
# Another factor is random sampling of the two alleles during the sequencing process. For example, even though your individual is heterozygous, it could be that the two alleles are not equally represented, especially so with low read depths.   

# Say for now that the probabilities are as follows:
# P(D|G=0/0) = Pr(2A+8T|A/A) = 0.05
# P(D|G=0/1) = Pr(2A+8T|A/T) = 0.3
# P(D|G=1/1) = Pr(2A+8T|T/T) = 0.45
# (Note that being likelihoods, the values do not necessarily add up to 1.)

# Then the formula becomes:
# Pr(G=0/0|D=2A+8T) = Pr(2A+8T|A/A)/sum(Pr_i)	= 0.05/0.8	= 1/16 
# Pr(G=0/1|D=2A+8T) = Pr(2A+8T|A/T)/sum(Pr_i)	= 0.3/0.8	= 6/16
# Pr(G=1/1|D=2A+8T) = Pr(2A+8T|T/T)/sum(Pr_i)	= 0.45/0.8	= 9/16

# In the vcf or bcf file this will be encoded as:
# GL	1/16,6/16,9/16



# THE CONVERSION TO PHRED LIKELIHOODS

# Because genotype likelihoods are often small values which are difficult to work with, a common practice is to transfer them to negative logs.
# A commonly used scale is the phred-scale, which multiplies the log with -10 rather than with -1.  

# Example:
# Say we have the following genotype likehoods; with A for reference and T for alternative allele:
# Pr(AA|Data) = 0.000001
# Pr(AT|Data) = 0.000100
# Pr(TT|Data) = 0.010000

# The Raw Phred-scaled likelihoods are:
# PL = -10 * log10(0.000001) = -10*-6 = 60	
# PL = -10 * log10(0.000100) = -10*-4 = 40	
# PL = -10 * log10(0.010000) = -10*-2 = 20

# The normalized Phred-scaled likelihoods are:
# PL = 60 - 20 = 40	
# PL = 40 - 20 = 20	
# PL = 20 - 20 = 0

# In vcf file it will be coded as:
# GT:PL		1/1:40,20,0
# This shows that genotype 1/1 is 100x (10^-2/10^-4) less likely than genotype 0/1, and that in turn genotype 0/1 is 100x less likely than genotype 1/1. 

# Now why this does normalization procedure work so neatly?
# If you have previously worked with likelihoods, you may know that likelihood scores are often normalized by dividing the scores by the sum of all scores, so that the sum of the normalized scores equals 1.
# How does this compare to the normalization of Phred-scales likelihoods by subtracting the lowest values? 
# The reason this works (approximately at least), is that we are working with logtransformed values.
# When working with log-values, a division or multiplication operation (e.g. multiplying probabilities) can be done using subtraction or addition, with makes it easier and faster.
# For example: 
# P(AA | Data) = 0.000001/(0.000001+0.000100+0.010000) = 0.0000990001	= ~0.0001	= ~10^-4 	# -10*log10(0.0001) = 40
# P(AT | Data) = 0.000100/(0.000001+0.000100+0.010000) = 0.00990001		= ~0.01		= ~10^-2	# -10*log10(0.01) 	= 20
# P(TT | Data) = 0.010000/(0.000001+0.000100+0.010000) = 0.990001		= ~1		= ~10^0		# -10*log10(1)		= 0
# Although the normalization procedures does not produced exactly the same results (i.e. 10^0 is 1, not 0.990001), they are close enough.

# To summarize, PL values are phred-scaled likelihood scores, normalized such that the most likely genotype will have a score of 0. So the approximate likelihoods are 10^(-PL/10). 
# For example, for PL values of 103,0,26 the likelihoods would be:
# 10^(-10.3) approximately 5.0E-11
# 10^0 approximately 1
# 10^(-2.6) approximately 0.0025


# GENOTYPE QUALITY (GQ):
# The software GATK defines genotype quality (GQ) simply as the difference in PL between the lowest (0) and second lowest genotype likelihood. 
# In the example above, this equals (20-0=) 20. 
# The GQ score outputted by mpileup does not equal the difference between the lowest and second lowest genotype likelihood (although not much difference)

# Looking at my vcf file, I decided that I prefer not to filter on QC.
# For example, here are some genotype scores:
# 	GT		AD		GQ
#	0/0		11:1	8
#	0/1		1:19	25
#	1/1		0:13	18


	
	



# VCF FORMAT FIELD DESCRIPTION:

# GT: genotypes For example: 0/1 or 0|1
# DP: read depth at this position for this sample
# GL: genotype likelihoods. For biallelic sites: AA,AB,BB. For triallelic sites: AA,AB,BB,AC,BC,CC. For example: GT:GL 0/1:-323.03,-99.29,-802.53
# GP: the phred-scaled genotype posterior probabilities (and otherwise defined precisely as the GL field)
# GQ: conditional genotype quality, encoded as a phred quality â10log10 p(genotype call is wrong, conditioned on the site being variant). Scroll down for more details.
# EC: comma separated list of expected alternate allele counts for each alternate allele in the same order as
# MQ: RMS mapping quality, similar to the version in the INFO field
# PL: the phred-scaled genotype likelihoods rounded to the closest integer (and otherwise defined precisely as the GL field).

# AD and DP:
# AD is the unfiltered allele depth, i.e. the number of reads that support each of the reported alleles. 
# All reads at the position (including reads that did not pass the variant callerâs filters) are included in this number, except reads that were considered uninformative. 
# Reads are considered uninformative when they do not provide enough statistical evidence to support one allele over another.
# DP is the filtered depth, at the sample level. 
# This gives you the number of filtered reads that support each of the reported alleles. You can check the variant callerâs documentation to see which filters are applied by default. 
# Only reads that passed the variant callerâs filters are included in this number. However, unlike the AD calculation, uninformative reads are included in DP.





# VCF INFO FIELD DESCRIPTION:

# INFO FIELDS of vcf file created by bcftools mpileup/call pipeline:
# monomorphic:	DP=1505;MQSB=0.464167;MQ0F=0;AN=246;DP4=1246,100,0,0;MQ=48
# polymorphic:	DP=2547;VDB=0.530878;SGB=31.6031;RPB=0.844371;MQB=1.36783e-19;MQSB=0.796696;BQB=0.0418234;MQ0F=0;ICB=0.985275;HOB=0.00608104;AC=26;AN=246;DP4=1140,888,108,78;MQ=46

# What do they mean?

# Mapping quality (MQ)
# Mapping quality (MQ) of reads across all samples provides an estimation of the overall mapping quality to support a variant call. 
# Good mapping qualities are around MQ 60. 
# GATK recommends hard filtering of variants with MQ less than 40.

# MQ0Fraction = RMS 
# Also known as the quadratic mean Mapping Quality. 
# Root Mean Square of the mapping quality of reads across all samples.
# The root mean square is equivalent to the mean of the mapping qualities plus the standard deviation of the mapping qualities.
# however, not according to this example calculations? Maybe only if working with (phred log scales)?
# say we have a vector v, composed of numbers 5, 10 and 15.
# mean(v) 	= 1/3*(5+10+15)			= 10
# sd(v)		= 5
# RMS(v)	= sqrt(1/3*(5^2+10^2+15^2))	= 10.8 	
# next say we have a vector z, composed of numbers -5, 0 and 5.
# mean(z) 	= 1/3*(-5+0+5)				= 0
# RMS(z)	= sqrt(1/3*(-5^2+0^2+5^2))	= 4.1 	

# DP4
# Number of reference/alternative alleles on forward and reverse reads
# 1) forward ref alleles; 
# 2) reverse ref; 
# 3) forward non-ref; 
# 4) reverse non-ref alleles. 
# Sum can be smaller than DP because low-quality bases are not counted

# Mapping Quality versus Strand Bias (MQSB)
# ?

# BQB:
# ?


## not present in bcftools mpileup/call output:

# GC
# GC content within 20 bp +/- the variant

# HW 
# Phred-scaled p-value for Hardy-Weinberg violation. Extreme variations on heterozygous calls indicate a false positive call

# HRun
# Largest Contiguous Homopolymer Run of Variant Allele In Either Direction

# SB 
# Strand Bias

# Fisher Strand (FS) 
# Phred-scaled p-value using Fisher's exact test to detect strand bias. If the reads carrying the reference allele are balanced between forward and reverse strands, then the reads carrying alternative reads should be as well.
# In other words, the FS score tells us whether the alternate allele was seen more or less often on the forward or reverse strand than the reference allele. 
# A value close to 0 indicates little or no strand bias. 
# GATK recommends hard filtering of variants with FS greater than 60. 
# This is however a very conservative value that will only filter out cases of extreme bias and leave many false positives in the data set.

# Strand Odds Ratio (SOR) 
# Also a measurement for strand bias, but it takes into account the ratios of reads that cover both alleles.
# It is therefore a better measurement for sites that have more reads in one direction than the other. 
# SOR values range from 0 to greater than 9, a value close to 0 indicates little or no strand bias. 
# GATK recommends hard filtering of variants with SOR greater than 3.

# Mapping Quality Rank Sum Test (MQRankSum) 
# The Mapping Quality Rank Sum Test (MQRankSum) compares the mapping qualities of reads supporting the reference and the alternate allele.
# MQRankSum = Z-score from Wilcoxon rank sum test of Alt vs. Ref read mapping qualities. 
# If the alternate bases are more likely to be found on reads with lower MQ than reference bases then the site is likely mismapped.
# Negative values indicate a lower MQ of the reads supporting the alternate allele, compared to reads supporting the reference allele. 
# If all reads map correctly, the expected MQRankSum is 0.
# MQRankSum values usually range from about -10.5 to 6.5. 
# GATK recommends hard filtering of extreme outlier variants with MQRankSum less than -12.5.

# Quality by depth (QD) 
# The QUAL score normalized by the read depth at that site. 
# GATK recommends hard filtering of variants with QD below 2.

# ReadPosRankSum
# The Read Position Rank Sum Test (ReadPosRankSum) measures the relative position of the reference versus the alternative allele within reads. 
# E.g., alternate bases towards the ends of reads may be more likely a sequencing error or mapping artifact.
# ReadPosRankSum = Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias. 
# A value around 0 means there is little to no difference in where the alleles are found relative to the ends of reads. 
# GATK recommends hard filtering of variants with ReadPosRankSum less than -8.0.

# BaseQualityRankSumTest
# BaseQualityRankSumTest = The u-based z-approximation from the Mann-Whitney Rank Sum Test for base qualities (ref bases vs.bases of the alternate allele).
