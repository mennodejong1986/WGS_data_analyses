#!/bin/bash
# This script is for rapid genotype calling/filtering using bcftools mpileup, bcftools call, and bcftools filter
# Input: bam files (indexed with samtools). 
# Output: vcf file.

# The mpileup command is run with the flags --min-MQ 20 and -min-BQ 13
# Bam-files should be with readgroups (PICARD addorreplace readgroups), with duplicates removed (PICARD markduplicates), and indexed (samtools index).
# For more info, see script: FASTQ2BAM_bwaloop.sh.  

# In principle you could invoke all steps in one go by setting all flags in the user-defined section to TRUE.
# However, I recommend to proceed stepwise, and to only proceed to the next step only after checking if intermediate output files look okay and no errors have occurred.
# Also, based on depth and heterozygosity statistics generated by this script, you might want to customize filter settings. 

# Also, make sure you have enough storage space.
# As an indication: 120 brown bear whole genome samples resulted in just under 1 TB of BCF files. 
# Because these files needs to be combined into a single bcf file before they can be deleted, for this dataset up to 2 TB free space is required.   

# Three main output files:
# - allsites.globalfilter.vcf.gz			# contains all sites (monomorphic and polymorphic) retained after global filtering 
# - mysnps.missfilter.vcf.gz				# contains biallelic sites after filtering
# - mysnps.thinned.vcf.gz					# contains random subset of biallelic sites (obtained by thinning: selecting 1 site per 50Kb (default))

# Note, if running the analysis for multiple samples (i.e. multiple bam files), it is crucial that each bam file contains a unique SM tag in the header of the bam file.
# If you do not have unique SM tags, you will end up with a vcf file containing information for one individual only

# Normally, unique SM tags are added with picard AddOrReplaceReadGroups.
# An alternative easy and fast way to edit the SM tags is samtools reheader. Say you want to change the SM tag from 'sample' to 'human_1', the command would be:
# samtools view -H file.bam > mybamheader.txt
# sed "s/SM:sample/SM:human_1/g" mybamheader.txt
# samtools reheader mybamheader.txt file.bam > file.newheader.bam

# QUICK START GUIDE
# To create the allsites.globalfilter.vcf.gz file, go through the following steps:
# 0. make the script an executable (chmod +x BAM2VCF_run_mpileup_parallel_HIGHWAY.sh) and remove potential unwanted hidden characters using dos2unix command (dos2unix BAM2VCF_run_mpileup_parallel_HIGHWAY.sh)
# 1. In user-defined section, define paths to software, reference genome and a txt file which lists the input bam files. 
# 2. In user-defined section, define the desired number of sets (i.e, number of parallel runs) and the prefix of the output bcf file.
# 3. Set the flag createbed to TRUE (leave all other FALSE) and execute the script
# 4. Observe the file 'bedfiles.nrsites.txt' to see if sites are relatively equally distributed, and if there are no empty bed-files (if there are empty bed files, remove them.) Optionally, split big files in a and b (e.g. mybed9a.txt and mybed9b.txt).
# 5. If everything looks okay, set the flag runmpileup to tree (and all other flags to FALSE). Execute the script (this can take hours to days, depending on data size).
# 6. Next, go one by one through the steps 'combinebcf', 'indexbcf' and 'callgenotypes'.
# 7. Next, you can skip the statistics section and directly move on to the genotype masking section. First run 'maskgenotypes', afterwards 'combinevcf'.
# 8. To run the global filter, optionally first run 'calcdepth' in the statistics section. Use the script 'VCF_plotdepth.inR.txt' to visualize the output.
# 9. Based on information acquired at step 8, set mindepth and maxdepth, and then run the 'globalfilter' step.
# 10. The output will be a file called 'PREFIX.allsites.globalfilter.vcf.gz'. 




########################  USER-DEFINED SECTION ##################################

SAMTOOLS=samtools									# Path to samtools executable
#BCFTOOLS=/opt/bcftools/bcftools/bcftools						# Path to bcftools executable, at least: git version 1.11-24-g9718479+, older versions do not contain bcftools call --group-samples option
BCFTOOLS=/opt/software/bcftools-1.18/bcftools
VCFTOOLS=/opt/software/vcftools/vcftools_0.1.17/bin/vcftools				# Path to vcftools executable
PLINK=/home/mdejong/software/plinkv2020-09-21/plink					# Path to plink executable (optional, only needed for convert2ped step)

REFERENCE=/home/mdejong/bearproject/refgenome/brownbear_chrom/ASM358476v1_HiC.fasta	# Path to unzipped reference genome.
BAMFILES=my112bamfiles.NorthAmerica.txt							# a txt file with (full path and) names of input bam files, one per line. Bam-files should already be indexed with samtools index command.
POPFILE=my112groupfile.NorthAmerica.txt							# necessary for callgenotypes step; tab separated txt-file with two columns (no header): sample name and population name. 
											# Note: this should not correspond to bam-file name, but to SM-tag in bam-file (specified with addreadgroup-command. 
											# For example: 'sample1.sorted.bam' could become simply 'sample1' (check the SM tag).
											# I recommend: group all samples apart, meaning: column2 identical to column1. 

NRSETS=50			# Number of subdivisions (this is basically the number of commands you want to run parallel). 
				# Note: sometimes when running createbed you might get the error: Error: mybed.txt: No such file or directory. In that case choose another (lower?) value for NRSETS and try again.
PREFIX=NorthAmericamodern112	# Desired prefix of output files 
PLOIDY=2                        # Only used if setploidy is set to FALSE (default). Set to 1 for mt-DNA data.
setploidy=FALSE                 # only set to true if you want to define ploidy level dependent on gender of sample (e.g. for Y-chrom or X-chrom). If true, define ploidyfile and samplefile:
PLOIDYFILE=ploidyfile.txt       # needed if setploidy=TRUE for callgenotypes step; tab-separated txt-file with 5 columns: chrom, start, end, gender, ploidy
SAMPLEFILE=mysamples.txt        # needed if setploidy=TRUE for callgenotypes step; tab-separated txt-file with 2 columns (no header): sample name and gender (M or F)


# GENOTYPE CALLING:
createbed=TRUE
runpipeline=FALSE		## Note: either this option (bcftools mpileup, norm, call and filter (mask) all at once), or alternatively step by step (runmpileup, combinebcf, indexbcf, callgenotypes and maskgenotypes)  
				# 25 sets use around 50 cores in total.

runmpileup=FALSE		# Only set to TRUE if not using runpipeline (all at once) option
combinebcf=FALSE		# Only set to TRUE if not using runpipeline (all at once) option
indexbcf=FALSE			# Only set to TRUE if not using runpipeline (all at once) option
callgenotypes=FALSE		# Only set to TRUE if not using runpipeline (all at once) option
				# included in this step is also: multi nucleotide variant (MNV) left alignment and normalization using bcftools norm
				# Note: bcftools norm also has the option to split multi-allelic into bi-allelic, but this option is not activated by this script (because undesirable for me(nno's) purposes.

# STATISTICS:
calcdepth=FALSE			# Optional (and only if not using runpipeline option); can be used to determine optimal depth settings for masking (individual based) and globalfilter step
calche=FALSE			# Optional (and only if not using runpipeline option), can be used to determine optimal depth settings for masking (individual based) and globalfilter step	
calcsnpgap=FALSE		# Optional (and only if not using runpipeline option), can be used to determine optimal snpgap filter for globalfilter step

# GENOTYPE MASKING:
maskgenotypes=FALSE		# Only set to TRUE if not using runpipeline (all at once) option. Use the flag 'maskdepth' to set desired minimum number of reads per genotype
maskdepth=3			## By default, genotypes with read depth below 3 are masked.
combinevcf=FALSE		# Optional, as next step can also run on vcf file subsets.

# SITE FILTERING:
globalfilter=FALSE		## Runs either on vcf file subsets or combined vcf file. Use the flags mindepth and maxdepth to customize filter			
mindepth=550			## around 6x nrsamples (mindepth) 
maxdepth=2100			## around 1.5x overall meandepth (e.g: if mean depth per sample is 10, and you have 100 samples, then: 10*100*1.5 = 1500)
combinevcf2=FALSE		# Optional, as next step can also run on vcf file subsets.

# EXTRACT SNPS:
selectbiallelic=FALSE		## Runs either on vcf file subsets or combined vcf file.
combinevcf3=FALSE		## Combine subset vcf files. 
missfilter=FALSE		## Note: use minalleles flag (next line) to set missfilter
minalleles=200			## recommendation: in the range of >1.9*nrsamples in case of diploid data, and in the range of >0.95*nrsamples in case of haploid data.
thinvcf=FALSE			## Note: use the 'thinbp' flag (next line) to adjust the thinning parameter (i.e. minimum distance between snps in bp). Default is 1 SNP per 50Kb
thinbp=50000			##
calcsnpdepth=FALSE		##

# FILE CONVERSION:
convert2ped=FALSE		## Note: runs PLINK to convert vcf to PED/MAP and subsequently to RAW/BIM
convert2beagle=FALSE		# Note: not tested yet


# In a conda environment, I run into the error: File size limit exceeded (core dumped)
#################################################################################
# User shouldn't change anything from here.






if [[ "$createbed" = TRUE ]]
	then
	echo "Start creating bed files"
	${SAMTOOLS} faidx ${REFERENCE}							# find out total number of bp in your genome
	cut -f1-2 ${REFERENCE}.fai > contigs.bed
	awk '{ total += $2 } END { print total }' contigs.bed > nrsites.txt		# I find: 2607875777. We are going to divide this over NRSETS equal batches (by generating bedfiles, which serve as input for samtools mpileup)
	NRSITES=$(cat nrsites.txt)
	cut -f2 contigs.bed > column2.txt
	perl -lne 'print $sum+=$_' column2.txt > cum.txt
	paste -d '\t' contigs.bed cum.txt > contigs.cum.bed

	echo "Subdivisions:"
	seq 1 $NRSETS > mynumbers.txt
	for digit in $(cat mynumbers.txt)
	do
	num=$(($digit-1))
	part=$((${NRSITES}/${NRSETS}))
	var1=$((${num} * ${part}))
	var2=$((${digit} * ${part}))
	echo $digit
	echo $var1
	echo $var2
	awk -v mystart="${var1}" -v myend="${var2}" '{ if ( $3 >= mystart && $3 <= myend ) print > "mybed.tmp.txt" }' contigs.cum.bed
	if [ -f "mybed.tmp.txt" ]
		then
		cut -f1 mybed.tmp.txt > contignames.txt
		cut -f2 mybed.tmp.txt > contiglengths.txt
		sed -i 's/^/0___/' contiglengths.txt
		sed -i 's/___/\t/g' contiglengths.txt
		paste -d '\t' contignames.txt contiglengths.txt > mybed.tmp.txt
		awk '{ if ($2 == "0") $2=1; print $0 }' mybed.tmp.txt | sed 's/ /\t/g' > mybed${digit}.txt	# bcftools expects 1 based reference system
		rm mybed.tmp.txt
	fi	
	done
	rm mynumbers.txt nrsites.txt contig* cum.txt column2.txt

	echo "Creating overview of number of bp per subset..."
	ls -1 mybed*txt > bedfiles.nrsites.tmp1.txt
	if [ -f "bedfiles.nrsites.tmp2.txt" ]; then rm bedfiles.nrsites.tmp2.txt; fi
	touch bedfiles.nrsites.tmp2.txt 	
	for bedfile in mybed*txt
	do
	nrsites=$(awk '{sum+=$3;} END{print sum;}' $bedfile)
	echo $nrsites >> bedfiles.nrsites.tmp2.txt
	done
	paste bedfiles.nrsites.tmp1.txt bedfiles.nrsites.tmp2.txt > bedfiles.nrsites.txt
	rm bedfiles.nrsites.tmp1.txt bedfiles.nrsites.tmp2.txt

	echo "Finished creating bed files."
	echo "Note that the number of bed files might be lower than specified, because this script does not chop up chromosomes (and hence number of chromosomes may set an upper boundary)."
	echo "You can observe the file 'bedfiles.nrsites.txt' to see if sites are relatively equally distributed over subsets."
	echo "Often the last file (containing unplaced scaffolds) contains considerable more sites than the other files."
	echo "It might be worthwhile to split this file into multiple files otherwise this file may cause a delay." 
fi

if [[ "$runpipeline" = TRUE ]]
	then
	echo "Starting pipeline of bcftools mpileup, call, norm and filter (all at once)..."
	# bcftools mpileup uses 100%, whereas call, norm and filter use around 30%. So in total, per bed file, around 200% (i.e., 2 cores).
	for mybedfile in mybed*.txt
	do
	echo ${mybedfile}
	if [[ "$setploidy" = TRUE ]]
		then
		${BCFTOOLS} mpileup -A --min-MQ 20 -min-BQ 13 -C50 -a "DP,AD" -R ${mybedfile} --output-type u -f ${REFERENCE} --bam-list ${BAMFILES} | 
		${BCFTOOLS} call -a GQ -m --group-samples $POPFILE --output-type z --ploidy-file $PLOIDYFILE --samples-file $SAMPLEFILE | 				# this line is affected by setploidy flag 
		${BCFTOOLS} norm --check-ref w --fasta-ref ${REFERENCE} -O z |
		${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP<${maskdepth}" -O z -o masked_${PREFIX}.${mybedfile}.vcf.gz &
		else
		${BCFTOOLS} mpileup -A --min-MQ 20 -min-BQ 13 -C50 -a "DP,AD" -R ${mybedfile} --output-type u -f ${REFERENCE} --bam-list ${BAMFILES} | 
		${BCFTOOLS} call -a GQ -m --group-samples $POPFILE --output-type z --ploidy ${PLOIDY} | 												# this line is affected by setploidy flag
		${BCFTOOLS} norm --check-ref w --fasta-ref ${REFERENCE} -O z |
		${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP<${maskdepth}" -O z -o masked_${PREFIX}.${mybedfile}.vcf.gz &
	fi
	done
	wait
	echo "Finished bcftools mpileup/call/norm/filter pipeline."
	echo "You can directly proceed with or globalfilter step and skip all steps in between."
fi

if [[ "$runmpileup" = TRUE ]]
	then
	echo "Starting bcftools mpileup now."
	for mybedfile in mybed*.txt	
	do
	echo ${mybedfile}
	# base quality score of Illumina 1.8+ ranges from 0 to 41, and are denoted by, from low to high: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI. If using threshold of 13, any base with value on left hand side from / is deleted. 
	# Q: min-BQ: minimum base quality (default is 13, which roughly corresponds to a probability of 0.05 that base call is incorrect, as given by the formula -10*log10(0.05))
	# q: min-MQ: minimum mapping quality (default is 0)
	${BCFTOOLS} mpileup -A --min-MQ 20 -min-BQ 13 -C50 -a "DP,AD" -R ${mybedfile} --output-type b -f ${REFERENCE} --bam-list ${BAMFILES} > ${PREFIX}.${mybedfile}.bcf &
	# by default only PL in sample info columns format
	# with the -a flag (annotate flag) we add AD (allele depth) and DP (total depth), so that during bcftools call this information can be included in the sample genotype columns
	#
	## What about -B or --no-BAQ?
	## For low depth data (around 15x depth):
	# -B or --no-BAQ:	do NOT set this flag!! 
	# Specifying this option will disable probabilistic realignment for the computation of base alignment quality (BAQ), which greatly helps to reduce false SNPs caused by misalignments.
	# BAQ is the Phred-scaled probability of a read base being misaligned.
	# m: min-ireads: minimum number gapped reads needed to infer an indel. By default: 1. This threshold could be set a bit higher, e.g. 2 or 3.
	## For high depth data (around 60x depth): 
	# DO set --B but use a higher min-BQ threshold.
	# The reason is that the BAQ calculations have been written for low depth data, and the underlying algorithm is not suitable for high depth data.
	#
	done
	wait
	echo "Finished mpileup." 
fi

if [[ "$combinebcf" = TRUE ]]
	then
	echo "Listing and sorting bcf subset files..."
	ls ${PREFIX}*mybed*bcf > mybcfsubset.files.txt
	cut -f2 -d '.' mybcfsubset.files.txt | sed 's/mybed//g' > mybcfsubset.numbers.txt
	paste mybcfsubset.numbers.txt mybcfsubset.files.txt | sort -k1,1n | cut -f2 > mybcfsubset.files.sorted.txt
	rm mybcfsubset.files.txt mybcfsubset.numbers.txt
	echo "Start concatenating bcf files to one single file."
	${BCFTOOLS} concat --threads 20 --output-type b --file-list mybcfsubset.files.sorted.txt > ${PREFIX}.combined.bcf
	# ${BCFTOOLS} concat --threads 20 --output-type b ${PREFIX}.mybed*.bcf > ${PREFIX}.combined.bcf	
	# Even though number of threads is set to 20, I have only seen concat command using up to 800%.
	echo "Finished combining."
	# rm *mybed*bcf
	# echo "Removed intermediate bcf-files."
	# To view header:
	# bcftools view -h ${PREFIX}.combined.bcf | head -3
	# To view data:
	# bcftools view -H ${PREFIX}.combined.bcf  | head -3
fi

if [[ "$indexbcf" = TRUE ]]
	then
	echo "Indexing bcf-file..."
	${BCFTOOLS} index -f --threads 20  ${PREFIX}.combined.bcf
	echo "Finished indexing bcf-file."
fi

if [[ "$callgenotypes" = TRUE ]]
	then
	echo "Genotype calling using bcftools call..."
	for mybedfile in mybed*.txt	
		do
		echo ${mybedfile}
		if [[ "$setploidy" = TRUE ]]
			then
			echo "Assuming user-defined ploidy-levels..."
			${BCFTOOLS} call -a GQ -m -R ${mybedfile} --ploidy-file $PLOIDYFILE --samples-file $SAMPLEFILE --group-samples $POPFILE --output-type z ${PREFIX}.combined.bcf | ${BCFTOOLS} norm --check-ref w --fasta-ref ${REFERENCE} -O z > ${PREFIX}.${mybedfile}.vcf.gz &
			else
			echo "Assuming invariant ploidy-level for all scaffolds and all samples..."
			${BCFTOOLS} call -a GQ -m -R ${mybedfile} --ploidy ${PLOIDY} --group-samples $POPFILE --output-type z ${PREFIX}.combined.bcf | ${BCFTOOLS} norm --check-ref w --fasta-ref ${REFERENCE} -O z > ${PREFIX}.${mybedfile}.vcf.gz &
            fi
		# The bcftools norm command might return to the screen many lines starting with 'NON_ACGTN_REF'
		# If using the flag '--check-ref e' with e for error rather than w for warning, bcftools would abort. 
		#
		# BACKGROUND INFO:
		# Better not to multi-thread per file, because depending on the number of subsets you might overload the system (e.g 25x3=75 cores)
		# choose c (consensus) or m (multicaller) mode. Consensus is the original model, but multicaller mode (which considers more than 2 alleles per samples?) is now preferred.   
		# -p, --pval-threshold float: if in mode -c, accept variant if P(ref|D) < float. In words: assume alternative allele if probability of reference allele given the available data is below value
		# Do not use -v (--variants-only) flag, otherwise information on monomorphic sites will get lost, which makes it impossible to calculate measures such as He, pi and Dxy. 
		#
		# --annotate (-a) flag can add extra fields
		# Note, the --annotate flag has replaced the --format-fields flag:
		# -- format-fields (-f) can add GQ,GP fields (If you try to add other terms, you will receive an error)
		#
		# By default, genotype format of output vcf file:
		# GT:PL
		# If flag -a "DP,PL,AD" has been specified during mpileup run, then automatically it becomes:
		# GT:DP:AD			# if ALT is absent		e.g. 0/0:16:16
		# GT:PL:DP:AD		# if ALT is present		e.g. 0/0:0,12,108:4:4,0
		#
		# We add Genotype Quality (which can be used for filtering by vcftools). The genotype format becomes: 
		# GT:DP:AD			# if ALT is absent		e.g. 0/0:16:16
		# GT:PL:DP:AD:GQ	# if ALT is present		e.g. 0/0:0,12,108:4:4,0:127 
		done
	wait
	echo "Finished bcftools call for all subsets."
fi

if [[ "$calcdepth" = TRUE ]]
	then
	echo "Extracting depth information using vcftools..."
	${VCFTOOLS} --gzvcf masked_${PREFIX}.mybed1.txt.vcf.gz --depth --out sampledepth &
	#
	echo "Extracting sample names using bcftools query..."
	${BCFTOOLS} query --list-samples masked_${PREFIX}.mybed1.txt.vcf.gz > myinfo.samples.txt &
	echo "Thinning data using vcftools..."
	${VCFTOOLS} -c --thin 100 --gzvcf masked_${PREFIX}.mybed1.txt.vcf.gz --recode --recode-INFO-all | gzip > masked_${PREFIX}.mybed1.txt.thin100.vcf.gz
	echo "Extracting depth information from thinned dataset using bcftools query..."
	${BCFTOOLS} query -f '%DP[\t%DP]\n' masked_${PREFIX}.mybed1.txt.thin100.vcf.gz > sampleinfo.DP.txt
	${BCFTOOLS} query -f '%DP[\t%GQ]\n' masked_${PREFIX}.mybed1.txt.thin100.vcf.gz > sampleinfo.GQ.txt
	wait
	echo "Depth information (mean per sample) stored in file called 'sampledepth.depth.out'."
	echo "Depth information (subset of sites) stored in file called 'sampledepth.info.txt'."
fi

if [[ "$calche" = TRUE ]]
	then	
	echo "Masking sites using various depth thresholds and subsequently calculating heterozygosity..."
	echo "Depths 1 to 15..."
	for k in $(seq 1 15)
		do
        	${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP!=$k" ${PREFIX}.mybed1.txt.vcf.gz -O z | ${BCFTOOLS} stats -s- --threads 1 > bcfstats.out.depth${k}.txt &
        	done
	wait
	#
	echo "Depths 16 to 30 and higher..."
	for k in $(seq 16 30)
		do
		${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP!=$k" ${PREFIX}.mybed1.txt.vcf.gz -O z | ${BCFTOOLS} stats -s- --threads 1 > bcfstats.out.depth${k}.txt & 
		done
	${BCFTOOLS} filter --threads 1 --set-GTs . -e "FMT/DP<31" ${PREFIX}.mybed1.txt.vcf.gz -O z | ${BCFTOOLS} stats -s- --threads 1 > bcfstats.out.depth31.txt &
	wait
	#
	echo "Extracting information..."
	for k in $(seq 1 31)
        do
		# 06-07-2021: I made the numerator ($4+1) rather than $4, to prevent zero values which causes awk to behave weird
		grep 'PSC' bcfstats.out.depth${k}.txt | cut -f3-6,14 | tail -n +3 | awk -v OFS='\t' '$6=($4+1)/($2+$3+$4)' > bcfstats.he.depth${k}.txt 
		sed -i "1i name    nrefhomo        nalthomo        nhet    nmiss   he" bcfstats.he.depth${k}.txt & 
		done
	
	wait
	echo "Done"
	echo "Results stored in files called 'bcfstats.he.depth{..}.txt."
	echo "Observe output (using the R script VCF_plotdepth.inR.txt') to choose an appropriate value for masking sites." 
fi

if [[ "$calcsnpgap" = TRUE ]]
        then
        echo "Calculating heterozygosity using different snpgap filter settings..."
        for k in 1 2 3 4 5 6 7 8 9 10 20 30 50 
                do
                ${BCFTOOLS} filter --SnpGap $k ${PREFIX}.mybed1.txt.vcf.gz --threads 1 --output-type z | ${BCFTOOLS} stats -s- --threads 1 > bcfstats.out.mybed1.snpgap${k}.txt &
                done
        wait
        echo "Extracting information..."
        for k in 1 2 3 4 5 6 7 8 9 10 20 30 50
                do
                grep 'PSC' bcfstats.out.mybed1.snpgap${k}.txt | cut -f3-6,14 | tail -n +3 | awk -v OFS='\t' '$6=$4/($2+$3+$4)' > bcfstats.he.snpgap${k}.txt
                sed -i "1i name    nrefhomo        nalthomo        nhet    nmiss   he" bcfstats.he.snpgap${k}.txt
                done
        wait
        echo "Done"
        echo "Results stored in files called 'bcfstats.he.snpgap{..}.txt."
        echo "Observe output (using the R script VCF_plotdepth.inR.txt' to choose an appropriate value for setting distance to indels when running globalfilter."
fi

if [[ "$maskgenotypes" = TRUE ]]
	then
	echo "Masking sites with a read depth below 3 (or any other value set by user)..."
	ls ${PREFIX}.mybed*.txt.vcf.gz > myvcfsubset.files.txt
	for myvcffile in $(cat myvcfsubset.files.txt)	
		do
		echo ${myvcffile}
		#
		#### SAME SETTINGS FOR ALL SAMPLES
		# OPTION A:
		# $VCFTOOLS --gzvcf $myvcffile -c --minDP 3 --recode --recode-INFO-all | gzip > masked_${myvcffile} &
		# OPTION B:
		$BCFTOOLS filter --threads 1 --set-GTs . -e "FMT/DP<${maskdepth}" $myvcffile -O z -o masked_${myvcffile} &
		# --set-GTs or --S flag:	We set failed genotypes to missing (.). (Other option would be to set to reference: --set-GTs 0) 
		# Note: if using -S option, and if wanting to include multiple criteria, denote OR by |, not by ||. For example: -e 'FMT/DP<5 | FMT/GQ<10'
		#
		# Which read depth (DP) and/or genotype quality (GQ) filter should you choose?
		# Scroll down for more information about the meaning and the calculation of the GQ score.
		# According to online tutorials, if the groups defined to the flag --group-samples contain reasonable numbers of samples, we can safely keep genotypes with read depth as low as 3. 
		# The justification is that bcftools calls genotypes across all samples belong to the same group simultaneously, and includes the information from all reads from all individuals to adjust the genotype quality. 
		# A call with DP=3 may thus have a high GQ and can be confidently kept.
		# To account for groups containing 1 sample only, I decided to set read depth to 5.
		#
		### DIFFERENT SETTINGS PER SAMPLE SUBSET
		# If you want to use different thresholds depending on the sample, you have to first subset the input vcf files, and then later recombine.
		# Here assuming that you want to divide your samples in 3 different sets (which each unique mask settings):
		# ${BCFTOOLS} view --threads 1 --samples-file 2samples.txt $myvcffile | ${BCFTOOLS} filter --threads 1 --set-GTs . -e 'FMT/DP>20 | FMT/DP<4' -O z -o masked_${myvcffile}_1 &
		# ${BCFTOOLS} view --threads 1 --samples-file 2samples.txt $myvcffile | ${BCFTOOLS} filter --threads 1 --set-GTs . -e 'FMT/DP>20 | FMT/DP<4' -O z -o masked_${myvcffile}_2 &
		# ${BCFTOOLS} view --threads 1 --samples-file 2samples.txt $myvcffile | ${BCFTOOLS} filter --threads 1 --set-GTs . -e 'FMT/DP>20 | FMT/DP<4' -O z -o masked_${myvcffile}_3 &
		done
	wait
	echo "Finished masking. Output is stored in files called 'masked_{..}.txt.vcf.gz'."
fi

if [[ "$combinevcf" = TRUE ]]
	then
	echo "Creating sorted list of vcf files..."
	ls masked_${PREFIX}*mybed*.txt.vcf.gz > myvcfsubset.files.txt
	cut -f2 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
	paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
	rm myvcfsubset.files.txt myvcfsubset.numbers.txt
	echo "Combining (i.e. concatenating) vcf files..."
	${BCFTOOLS} concat --threads 40 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.masked.vcf.gz
	wait
	echo "Finished combining vcf files. Output is stored in file 'PREFIX.masked.vcf.gz' (previously 'PREFIX.combined.vcf.gz')."
fi

# three options for filter (e.g. read depth):
# - global soft filter:			remove a site (entire line) if overall read depth (all samples combined) falls outside a specified range
# - global hard filter:			remove a site (entire line) if one or more samples fall outside a specified range
# - mask filter:				set a genotype to missing (./.) if read depth for this particular site and for this particular individual falls outside a specified range, and then filter on number of missing data points  		

# GLOBAL HARD FILTER:
# This is a very strict filter, which removes an entire site even if only single individual does not meet the requirements.
# Instead, I choose for a more relaxed approached, which consists of masking specific individuals (see 'mask-genotypes' step) and then filter sites on percentage of missing data.
# So the following line, even though shown here, should not be used (which means: do not remove the hashtag).
# $BCFTOOLS view  -i  'MIN(FMT/DP)>=15 & MAX(FMT/DP)<=45' ${PREFIX}.combined.vcf.gz -O z -o allsites.vcf.gz &

# One-liners to run the sections filtervcf, thinvcf and convert2ped yourself (without executing this script):
#/opt/software/bcftools/bcftools-1.9/bin/bcftools filter --threads 10 -i "QUAL>=30 && DP>=1000 && DP<=5000"  bears121_december2020.combined.vcf.gz -O z -o allsites.vcf.gz &
#/opt/software/bcftools/bcftools-1.9/bin/bcftools view --threads 10 --min-alleles 2 --max-alleles 2 --exclude-types indels -O z allsites.vcf.gz -o mysnps.vcf.gz &
#/opt/software/bcftools/bcftools-1.9/bin/bcftools view --threads 10 -i "AN>=242" -O z mysnps.vcf.gz -o mysnps.missfilter.vcf.gz &
#/opt/software/vcftools/vcftools_0.1.17/bin/vcftools --thin 50000 --gzvcf mysnps.missfilter.vcf.gz --recode --recode-INFO-all --out mysnps.thinned & 
#/opt/software/vcftools/vcftools_0.1.17/bin/vcftools --vcf mysnps.thinned.recode.vcf --plink --out mysnps &

if [[ "$globalfilter" = TRUE ]]
	then
	# Read depth (DP):
	# Observe output of getstats step to make an informed decision about read depth filter settings, customized to your dataset.
	# Rough guidelines: 
	# Minimum depth: around 6 times nrsamples. 
	# Maximum depth: maximum 1.5x mean depth (higher depth could indicate a paralogous locus, or telomeres and centromeres).
	#
	# Site quality (6th column):
	# For now, we want to find a dataset of reliable sites, both monomorphic and polymorphic, and we do not care about the ratio between reference and alternative alleles.  
	# Therefore, I (Menno) decided to not filter on QUAL at this step.
	#
	# SnpGap:
	# No snpGap filter, because I didn't find evidence for elevated levels of heterozygosity close to indels.
	#
	# Uses up to 5 (?) cores, even if you specify more threads.
	echo "Filtering vcf-file using a global filter..."
	if [ -f masked_${PREFIX}.mybed1.txt.vcf.gz ]; 
		then
		echo "Running analysis per subset..."
		for mybedfile in mybed*.txt
			do
			echo ${mybedfile}
			$BCFTOOLS filter --threads 3 -i "INFO/DP>=${mindepth} && INFO/DP<=${maxdepth}" masked_${PREFIX}.${mybedfile}.vcf.gz -O z -o ${PREFIX}.globalfilter.${mybedfile}.vcf.gz &
			done
		wait
		echo "Finished global filter. Output is stored in the files 'PREFIX.globalfilter.mybed1.txt.vcf.gz' etc."
		echo "Proceed either with combining vcf files (using combinevcf2 flag) or alternatively with SNP calling on subsets (selectbiallelic flag)."
		else
		if [ -f ${PREFIX}.masked.vcf.gz ];
			then
			echo "Running analysis for combined vcf file."
			$BCFTOOLS filter --threads 10 -i "INFO/DP>=${mindepth} && INFO/DP<=${maxdepth}" ${PREFIX}.masked.vcf.gz -O z -o ${PREFIX}.allsites.globalfilter.vcf.gz
			wait
			echo "Finished global filter. Output is stored in the file 'PREFIX.allsites.globalfilter.vcf.gz'."
			else
			echo "ERROR: No input files detected. Cannot run analysis."
		fi
	fi
fi  

# Optional step (not strictly needed as next step can also run on subset vcf files.)
if [[ "$combinevcf2" = TRUE ]]
	then
	echo "Creating sorted list of vcf files..."
	ls ${PREFIX}.globalfilter.mybed*.txt.vcf.gz > myvcfsubset.files.txt
	cut -f3 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
	paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
	rm myvcfsubset.files.txt myvcfsubset.numbers.txt
	echo "Combining (i.e. concatenating) vcf files..."
	${BCFTOOLS} concat --threads 40 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.allsites.globalfilter.vcf.gz
	wait
	echo "Finished combining vcf files. Output is stored in file called 'PREFIX.allsites.globalfilter.vcf.gz'."
	echo "You can proceed with the selectbiallelic step."
fi

if [[ "$selectbiallelic" = TRUE ]]
	then
	# SELECT BIALLELIC SNPS:
	# Uses up to 5 (?) cores
	#
	# Site quality (6th column):
    # QUAL, 6th column, is the probability that alternative allele is present (in case of monomorphic site) or absent (in case of polymorphic site).
    # Consider the following DP4 scores (which indicates: depth for REF_forward, REF_reverse, ALT_forward, ALT_reverse; in other words number of forward and reverse reads with REF and ALT allele). 
	# If DP4 is 100,100,0,0 then site is likely called monomorphic with a high quality score.
    # If DP4 is 97,97,3,3 then site is likely called monomorphic with a low quality score.
    # If DP4 is 50,50,50,50 then site is likely called polymorphic with a high quality score.
    # This measure is useful for the purpose of extracting high quality SNPs.
	#
	echo "Selecting biallelic SNPs..."
	if [ -f ${PREFIX}.globalfilter.mybed1.txt.vcf.gz ]; 
		then
		echo "Running analysis per subset..."
		for mybedfile in mybed*.txt
			do
			echo ${mybedfile}
			$BCFTOOLS view --threads 2 --min-alleles 2 --max-alleles 2 --exclude-types indels -O z ${PREFIX}.globalfilter.${mybedfile}.vcf.gz | ${BCFTOOLS} filter -i "QUAL>=30" -O z --threads 2 -o ${PREFIX}.mysnps.${mybedfile}.vcf.gz &
			done
		wait
		echo "Finished selecting biallelic snps. Output is stored in the files 'PREFIX.mysnps.mybed1.txt.vcf.gz' etc."
		echo "Proceed with combining these files using combinevcf3 flag."
		else
		if [ -f ${PREFIX}.allsites.globalfilter.vcf.gz ];
			then
			echo "Selecting biallelic SNPs from combined vcf file."		
			$BCFTOOLS view --threads 10 --min-alleles 2 --max-alleles 2 --exclude-types indels -O z ${PREFIX}.allsites.globalfilter.vcf.gz | ${BCFTOOLS} filter -i "QUAL>=30" -O z --threads 10 -o ${PREFIX}.mysnps.vcf.gz
			wait
			echo "Finished selecting biallelic SNPs. Output is stored in the file 'mysnps.vcf.gz'."
			else
			echo "ERROR: No input files detected. Cannot run analysis."
		fi
	fi	
fi

if [[ "$combinevcf3" = TRUE ]]
	then
	if [ -f ${PREFIX}.mysnps.mybed1.txt.vcf.gz ]
		then
		echo "Creating sorted list of vcf files..."
		ls ${PREFIX}.mysnps.mybed*.txt.vcf.gz > myvcfsubset.files.txt
		cut -f3 -d '.' myvcfsubset.files.txt | sed 's/mybed//g' > myvcfsubset.numbers.txt
		paste myvcfsubset.numbers.txt myvcfsubset.files.txt | sort -k1,1n | cut -f2 > myvcfsubset.files.sorted.txt
		rm myvcfsubset.files.txt myvcfsubset.numbers.txt
		echo "Combining (i.e. concatenating) vcf files..."
		${BCFTOOLS} concat --threads 30 --file-list myvcfsubset.files.sorted.txt -O z -o ${PREFIX}.mysnps.vcf.gz
		wait
		echo "Finished combining vcf files. Output is stored in file called 'PREFIX.mysnps.vcf.gz'."
		echo "You can proceed with the missfilter step."
		else
		echo "ERROR: No input files (e.g. PREFIX.mysnps.mybed1.txt.vcf.gz) detected. Cannot combine."
	fi
fi

if [[ "$missfilter" = TRUE ]]
	then
	# FILTER ON MISSING DATA:
	# To next filter on missing data replace 1.9*NRSAMPLES with desired (rounded) number. Uses up to 5 (?) cores.
	echo "Filtering on missing data..."
	$BCFTOOLS view --threads 10 -i "AN>=${minalleles}" -O z ${PREFIX}.mysnps.vcf.gz -o ${PREFIX}.mysnps.missfilter.vcf.gz
	wait
	echo "Finished filtering on missing data. Output is stored in the file 'PREFIX.mysnps.missfilter.vcf.gz'."
fi

if [[ "$thinvcf" = TRUE ]]
	# This step can be used to shrink the dataset, so that it can be imported into R (for example).
    # I use thinning as replacement for LD filtering.
    # This is faster, less computational expensive, and as I see it more to the point: you only want to filter out SNP pairs which are in physical linkage, not linkage due to other factors such as selection.
    then
    echo "Thinning vcf file..."
    ${VCFTOOLS} --thin $thinbp --gzvcf ${PREFIX}.mysnps.missfilter.vcf.gz -c --recode --recode-INFO-all | gzip > ${PREFIX}.mysnps.thinned.${thinbp}.vcf.gz &
    wait
    echo "Finished thinning. Output is stored in the file 'PREFIX.mysnps.thinned.vcf.gz'."
fi

if [[ "$calcsnpdepth" = TRUE ]]
    then
    # Note: this works only if -a "DP" has been specified during bcftools mpileup run
    # If not, DP info field will be absent from sample genotype column (see 9th column, not 8th column, of vcf file)
    # If absent, output depth files will contain nan-values only.
    echo "Calculating depth per sample..."
    ${VCFTOOLS} --gzvcf ${PREFIX}.mysnps.thinned.${thinbp}.vcf.gz --depth --out ${PREFIX}.mysnps.thinned.${thinbp} &
    echo "Calculating depth per site..."
    ${VCFTOOLS} --gzvcf ${PREFIX}.mysnps.thinned.${thinbp}.vcf.gz --site-mean-depth --out ${PREFIX}.mysnps.thinned.${thinbp} &
    wait
    echo "Finished snp depth calculations. Results stored in files 'PREFIX.mysnps.thinned.idepth' and 'PREFIX.mysnps.thinned.ldepth.mean'."
fi

if [[ "$convert2ped" = TRUE ]]
    then
    echo "Converting vcf to ped/map..."
    ${VCFTOOLS} --gzvcf ${PREFIX}.mysnps.thinned.${thinbp}.vcf.gz --plink --out ${PREFIX}.mysnps.thinned.${thinbp}
    wait
    # edit first column of map file:
    cut -f2 ${PREFIX}.mysnps.thinned.${thinbp}.map | cut -f1 -d ':' > mycontigs.txt && cut -f2,3,4 ${PREFIX}.mysnps.thinned.${thinbp}.map > mymap.txt && paste mycontigs.txt mymap.txt > ${PREFIX}.mysnps.thinned.${thinbp}.map && rm mycontigs.txt mymap.txt
    wait
    echo "Converting ped/map to raw/bim..."
    ${PLINK} --file ${PREFIX}.mysnps.thinned.${thinbp} --allow-extra-chr --chr-set 95 --make-bed --recode A --out ${PREFIX}.mysnps.thinned.${thinbp}
    echo "Finished converting vcf file to ped/map and raw/bim format."
	echo "You are ready to run population-genetic analyses (for example using SAMBAR)."
    # an alternative is to use plink: plink --vcf mysnps.thinned.vcf.gz --recode --out mysnps 
    # /home/mdejong/software/plinkv2020-09-21/plink --vcf mysnps.thinned.vcf.gz --recode A --make-bed --out mysnps
fi

if [[ "$convert2beagle" = TRUE ]]
    then
    # Note: extracts from input file the biallelic sites only
    # By default, only considers first 25 scaffolds. Edit head -25 to contain more or less.
    grep -m1 -B2000000 '#CHROM' ${PREFIX}.mysnps.thinned.${thinbp}.vcf | grep '##contig' | cut -d ',' -f1 | cut -d '=' -f3 | head -25 > mysnps.scaffolds.txt
    for mychrom in $(cat mysnps.scaffolds.txt)
    do
    ${VCFTOOLS} --vcf ${PREFIX}.mysnps.thinned.${thinbp}.vcf --BEAGLE-PL --chr $mychrom --out ${PREFIX}.mysnps.thinned.${thinbp}.${mychrom}
    done
    wait
    echo "Finished converting vcf file :-)"
fi




############### EXPLANATION ##########

# SPLITTING GENOME IN EQUAL SECTIONS

# You might think an easy way to split the genome into regions is simply by dividing the number of contigs evenly.
# However, some contigs are way bigger than others. If the big contigs are clumped, you can get a very uneven distribution.
# For example, this is the distribution of sites I got when dividing the number of contigs by 4:
# 2,386,470,152
# 21,398,024
# 31,723,389
# 168,284,212
# With this distribution, the first mpileuploop will run forever, and the others will be finished fairly quickly.
# So this way you don't gain much time. Therefore we have to divide by number of sites rather than by contigs.

# the awk loop above execute these commands:
# Say that you specified to divide the genome in 16 regions:
# awk '{ if ($4 >= 0*2607875777/16 && $4 <= 1*2607875777/16 ) print > "mybed1.txt" }' contigs.cum.bed
# awk '{ if ($4 >= 1*2607875777/16 && $4 <= 2*2607875777/16 ) print > "mybed2.txt" }' contigs.cum.bed
# etc
# until
# awk '{ if ($4 >= 15*2607875777/16 && $4 <= 16*2607875777/16 ) print > "mybed16.txt" }' contigs.cum.bed



# DIFFERENT TYPE OF QUALITY SCORES/FILTERS

# Along the way from fastq via bam to vcf, there are several quality scores, and it is easy to get confused between them.
# Here an overview of the most important ones:

# - QUAL (fastq) 
#	name:		base quality score
#	where:		every 4th line in fastq, 11th column in SAM/BAM file
#	meaning:	QUAL, is the sequencing quality, which can be platform biased, e.g. Ion seemed to have lower QUAL compared to Illumina.
# 	formula:	?

# - MQ (bam) 
#	name:		mapping quality 
#	where:		5th column in SAM/BAM file; mean values per site are given in MQ-field in 8th column (INFO) of VCF  
#	meaning:	typically an indication of how unique the region's sequence is: the higher the MQ, the more unique the sequence, and to less regions the read could have mapped 
#	formula:	?

# -	QUAL (vcf)
#	name:		site quality (?)
#	where:		6th column in VCF file
#	meaning:	probability that alternative allele is present (in case of monomorphic site) or absent (in case of polymorphic site). 
# 	formula:	Let P be the probability of the sample having a variant allele. If ALT!='.', QUAL equals -10log10(1-P). If ALT=='.', QUAL equals -10log10(P). I expect it depends on DP4 (number of reference/alternative alleles). 
#	filter:		for snps, filter out sites in which quality is below 30, i.e. if probability is higher than 0.001 (1/1000) that site is incorrectly called polymorphic
#	command:	bcftools filter 'QUAL>=30' input.vcf.gz

# - GQ (bcf/vcf)
#	name:		genotype quality 
#	where:		GQ field of individual columns (10th and higher), as indicated by format column (9th column)
#	meaning:	difference in phred-scaled genotype likelihood (PL) between the lowest and second lowest genotype likelihood
#	formula:	scroll down for more info
#	filter:		I decided to mask genotypes based on read depth rather than on GQ (scroll down for justification)
#	command:	vcftools --gzvcf inputvcf.gz --minDP 6 [--minGQ ?]--recode --recode-INFO-all | gzip > masked_${myvcffile} & 

# -	FQ (vcf)
#	name:		consensus quality
#	where:		FQ-field in 8th column (INFO) of VCF
#	meaning:	If positive, FQ equals the phred-scaled probability of there being two or more different alleles.
#				If negative, FQ equals the minus phred-scaled probability of all chromosomes being identical.
# 				Notably, given one sample, FQ is positive at hets and negative at homs.
# 	usage:		generated by bcftools call -c (so consensus mode), not -m (multiallelic mode)
#				used by vcfutils vcf2fq to create quality scores in output fastq files  





### THE CALCULATION OF GENOTYPE LIKELIHOODS

# Genotype likelihoods are the probability of a genotype given the observed data.
# Likelihoods are defined as:
# Pr(H|O) means: probability of the hypothesis (H) given the observed data (O)
# For genotyping this becomes:
# Pr(G|D) means: probability of a genotype (e.g 0/0, 0/1, 1/1) given the observed sequence data (D)

# GATK uses the following formula (mpileup as well?), based on Bayes Rule:
# Pr(G|D) =	(Pr(G)*Pr(D|G))/(sum(P(G_i)*P(D|G_i))) 
# If using a flat prior probability (which is default and which means that at the start each genotype is considered equally likely), then the formula simplifies to:
# Pr(G|D) =	(Pr(D|G))/(sum(P(D|G_i))) 

# The numeration (top part of the division, Pr(D|G), basically means: 
# probability of obtaining the observed sequence data (more precisely: allelic depths) given the hypothesized genotype
# The denominator is the sum of all genotype probabilities (the i in the formula is meant to denote each different genotype)

# For example, say that you have 10 reads, of which 2 are A (as reference) and 8 are T (alternative). Then:
# P(D|G=0/0) = Pr(2A+8T|A/A)
# P(D|G=0/1) = Pr(2A+8T|A/T)
# P(D|G=1/1) = Pr(2A+8T|T/T)
# Unfortunately, I do not know right now how exactly these probabilities are calculated.
# However, I imagine that the calculation is based on genotype error rates.
# Say for example that the error rate is 0.1, it is not unthinkable that you would see some T's in the sequence data even though the individual is in reality homozygous alternative.
# Another factor is random sampling of the two alleles during the sequencing process. For example, even though your individual is heterozygous, it could be that the two alleles are not equally represented, especially so with low read depths.   

# Say for now that the probabilities are as follows:
# P(D|G=0/0) = Pr(2A+8T|A/A) = 0.05
# P(D|G=0/1) = Pr(2A+8T|A/T) = 0.3
# P(D|G=1/1) = Pr(2A+8T|T/T) = 0.45
# (Note that the values do not necessarily have to add up to 1.)

# Then the formula becomes:
# Pr(G=0/0|D=2A+8T) = Pr(2A+8T|A/A)/sum(Pr_i)	= 0.05/0.8	= 1/16 
# Pr(G=0/1|D=2A+8T) = Pr(2A+8T|A/T)/sum(Pr_i)	= 0.3/0.8	= 6/16
# Pr(G=1/1|D=2A+8T) = Pr(2A+8T|T/T)/sum(Pr_i)	= 0.45/0.8	= 9/16

# In the vcf or bcf file this will be encoded as:
# GL	1/16,6/16,9/16



# THE CONVERSION TO PHRED LIKELIHOODS

# Because genotype likelihoods are often small values which are difficult to work with, a common practice is to transfer them to negative logs.
# A commonly used scale is the phred-scale, which multiplies the log with -10 rather than with -1.  

# Example:
# Say we have the following genotype likehoods; with A for reference and T for alternative allele:
# Pr(AA|Data) = 0.000001
# Pr(AT|Data) = 0.000100
# Pr(TT|Data) = 0.010000

# The Raw Phred-scaled likelihoods are:
# PL = -10 * log10(0.000001) = -10*-6 = 60	
# PL = -10 * log10(0.000100) = -10*-4 = 40	
# PL = -10 * log10(0.010000) = -10*-2 = 20

# The normalized Phred-scaled likelihoods are:
# PL = 60 - 20 = 40	
# PL = 40 - 20 = 20	
# PL = 20 - 20 = 0

# In vcf file it will be coded as:
# GT:PL		1/1:40,20,0
# This shows that genotype 1/1 is 100x (10^-2/10^-4) less likely than genotype 0/1, and that in turn genotype 0/1 is 100x less likely than genotype 1/1. 

# Now why this does normalization procedure work so neatly?
# If you have previously worked with likelihoods, you may know that likelihood scores are often normalized by dividing the scores by the sum of all scores, so that the sum of the normalized scores equals 1.
# How does this compare to the normalization of Phred-scales likelihoods by subtracting the lowest values? 
# The reason this works (approximately at least), is that we are working with logtransformed values.
# When working with log-values, a division or multiplication operation (e.g. multiplying probabilities) can be done using subtraction or addition, with makes it easier and faster.
# For example: 
# P(AA | Data) = 0.000001/(0.000001+0.000100+0.010000) = 0.0000990001	= ~0.0001	= ~10^-4 	# -10*log10(0.0001) = 40
# P(AT | Data) = 0.000100/(0.000001+0.000100+0.010000) = 0.00990001		= ~0.01		= ~10^-2	# -10*log10(0.01) 	= 20
# P(TT | Data) = 0.010000/(0.000001+0.000100+0.010000) = 0.990001		= ~1		= ~10^0		# -10*log10(1)		= 0
# Although the normalization procedures does not produced exactly the same results (i.e. 10^0 is 1, not 0.990001), they are close enough.

# To summarize, PL values are phred-scaled likelihood scores, normalized such that the most likely genotype will have a score of 0. So the approximate likelihoods are 10^(-PL/10). 
# For example, for PL values of 103,0,26 the likelihoods would be:
# 10^(-10.3) approximately 5.0E-11
# 10^0 approximately 1
# 10^(-2.6) approximately 0.0025


# GENOTYPE QUALITY (GQ):
# The software GATK defines genotype quality (GQ) simply as the difference in PL between the lowest (0) and second lowest genotype likelihood. 
# In the example above, this equals (20-0=) 20. 
# The GQ score outputted by mpileup does not equal the difference between the lowest and second lowest genotype likelihood (although not much difference)

# Looking at my vcf file, I decided that I prefer not to filter on QC.
# For example, here are some genotype scores:
# 	GT		AD		GQ
#	0/0		11:1	8
#	0/1		1:19	25
#	1/1		0:13	18


	
	



# VCF FORMAT FIELD DESCRIPTION:

# GT: genotypes For example: 0/1 or 0|1
# DP: read depth at this position for this sample
# GL: genotype likelihoods. For biallelic sites: AA,AB,BB. For triallelic sites: AA,AB,BB,AC,BC,CC. For example: GT:GL 0/1:-323.03,-99.29,-802.53
# GP: the phred-scaled genotype posterior probabilities (and otherwise defined precisely as the GL field)
# GQ: conditional genotype quality, encoded as a phred quality −10log10 p(genotype call is wrong, conditioned on the site being variant). Scroll down for more details.
# EC: comma separated list of expected alternate allele counts for each alternate allele in the same order as
# MQ: RMS mapping quality, similar to the version in the INFO field
# PL: the phred-scaled genotype likelihoods rounded to the closest integer (and otherwise defined precisely as the GL field).

# AD and DP:
# AD is the unfiltered allele depth, i.e. the number of reads that support each of the reported alleles. 
# All reads at the position (including reads that did not pass the variant caller’s filters) are included in this number, except reads that were considered uninformative. 
# Reads are considered uninformative when they do not provide enough statistical evidence to support one allele over another.
# DP is the filtered depth, at the sample level. 
# This gives you the number of filtered reads that support each of the reported alleles. You can check the variant caller’s documentation to see which filters are applied by default. 
# Only reads that passed the variant caller’s filters are included in this number. However, unlike the AD calculation, uninformative reads are included in DP.





# VCF INFO FIELD DESCRIPTION:

# INFO FIELDS of vcf file created by bcftools mpileup/call pipeline:
# monomorphic:	DP=1505;MQSB=0.464167;MQ0F=0;AN=246;DP4=1246,100,0,0;MQ=48
# polymorphic:	DP=2547;VDB=0.530878;SGB=31.6031;RPB=0.844371;MQB=1.36783e-19;MQSB=0.796696;BQB=0.0418234;MQ0F=0;ICB=0.985275;HOB=0.00608104;AC=26;AN=246;DP4=1140,888,108,78;MQ=46

# What do they mean?

# Mapping quality (MQ)
# Mapping quality (MQ) of reads across all samples provides an estimation of the overall mapping quality to support a variant call. 
# Good mapping qualities are around MQ 60. 
# GATK recommends hard filtering of variants with MQ less than 40.

# MQ0Fraction = RMS 
# Also known as the quadratic mean Mapping Quality. 
# Root Mean Square of the mapping quality of reads across all samples.
# The root mean square is equivalent to the mean of the mapping qualities plus the standard deviation of the mapping qualities.
# however, not according to this example calculations? Maybe only if working with (phred log scales)?
# say we have a vector v, composed of numbers 5, 10 and 15.
# mean(v) 	= 1/3*(5+10+15)			= 10
# sd(v)		= 5
# RMS(v)	= sqrt(1/3*(5^2+10^2+15^2))	= 10.8 	
# next say we have a vector z, composed of numbers -5, 0 and 5.
# mean(z) 	= 1/3*(-5+0+5)				= 0
# RMS(z)	= sqrt(1/3*(-5^2+0^2+5^2))	= 4.1 	

# DP4
# Number of reference/alternative alleles on forward and reverse reads
# 1) forward ref alleles; 
# 2) reverse ref; 
# 3) forward non-ref; 
# 4) reverse non-ref alleles. 
# Sum can be smaller than DP because low-quality bases are not counted

# Mapping Quality versus Strand Bias (MQSB)
# ?

# BQB:
# ?


## not present in bcftools mpileup/call output:

# GC
# GC content within 20 bp +/- the variant

# HW 
# Phred-scaled p-value for Hardy-Weinberg violation. Extreme variations on heterozygous calls indicate a false positive call

# HRun
# Largest Contiguous Homopolymer Run of Variant Allele In Either Direction

# SB 
# Strand Bias

# Fisher Strand (FS) 
# Phred-scaled p-value using Fisher's exact test to detect strand bias. If the reads carrying the reference allele are balanced between forward and reverse strands, then the reads carrying alternative reads should be as well.
# In other words, the FS score tells us whether the alternate allele was seen more or less often on the forward or reverse strand than the reference allele. 
# A value close to 0 indicates little or no strand bias. 
# GATK recommends hard filtering of variants with FS greater than 60. 
# This is however a very conservative value that will only filter out cases of extreme bias and leave many false positives in the data set.

# Strand Odds Ratio (SOR) 
# Also a measurement for strand bias, but it takes into account the ratios of reads that cover both alleles.
# It is therefore a better measurement for sites that have more reads in one direction than the other. 
# SOR values range from 0 to greater than 9, a value close to 0 indicates little or no strand bias. 
# GATK recommends hard filtering of variants with SOR greater than 3.

# Mapping Quality Rank Sum Test (MQRankSum) 
# The Mapping Quality Rank Sum Test (MQRankSum) compares the mapping qualities of reads supporting the reference and the alternate allele.
# MQRankSum = Z-score from Wilcoxon rank sum test of Alt vs. Ref read mapping qualities. 
# If the alternate bases are more likely to be found on reads with lower MQ than reference bases then the site is likely mismapped.
# Negative values indicate a lower MQ of the reads supporting the alternate allele, compared to reads supporting the reference allele. 
# If all reads map correctly, the expected MQRankSum is 0.
# MQRankSum values usually range from about -10.5 to 6.5. 
# GATK recommends hard filtering of extreme outlier variants with MQRankSum less than -12.5.

# Quality by depth (QD) 
# The QUAL score normalized by the read depth at that site. 
# GATK recommends hard filtering of variants with QD below 2.

# ReadPosRankSum
# The Read Position Rank Sum Test (ReadPosRankSum) measures the relative position of the reference versus the alternative allele within reads. 
# E.g., alternate bases towards the ends of reads may be more likely a sequencing error or mapping artifact.
# ReadPosRankSum = Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias. 
# A value around 0 means there is little to no difference in where the alleles are found relative to the ends of reads. 
# GATK recommends hard filtering of variants with ReadPosRankSum less than -8.0.

# BaseQualityRankSumTest
# BaseQualityRankSumTest = The u-based z-approximation from the Mann-Whitney Rank Sum Test for base qualities (ref bases vs.bases of the alternate allele).







